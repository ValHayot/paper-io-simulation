\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}
\usepackage{hyperref}

\algblock{Input}{EndInput}
\algnotext{EndInput}
\algblock{Output}{EndOutput}
\algnotext{EndOutput}
\newcommand{\Desc}[2]{\State \makebox[2em][l]{#1}#2}

\newcommand{\tristan}[1]{\color{orange}\textbf{From Tristan:}#1\color{black}}


\begin{document}
\title{I/O simulation model with Linux page cache, integration and evaluation in SimGrid framework}

\author{\IEEEauthorblockN{Hoang-Dung Do, Val\'erie Hayot-Sasson, Tristan Glatard
  }\\
  \IEEEauthorblockA{
    Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada
  }
}

\maketitle

	\begin{abstract}
		\begin{itemize}
			\item The I/O bottleneck in HPC and the need of experiments.
			\item HPC experiment frameworks and advantages of SimGrid.
			\item The missing of the ability to simulate page cache, the goal of this paper.
			\item Principle of the simulator, experiment scenarios and comparisons.
			\item Brief discussion on results and future work.
		\end{itemize}
	\end{abstract}

	\section{Introduction}
		\begin{itemize}
			\item HPC, the bottleneck in I/O and the demand of HPC experiments. 
			\item Difficulties in conducting high performance computing experiments and the need of simulation frameworks.
			\item Existing experiment methods, simulators, simulation frameworks. The advantages of SimGrid compared to others \cite{casanova2008, lebre2015}. The missing of the ability to simulate page cache in SimGrid \cite{lebre2015}.
			\item The objective of the paper: Add capability to simulate I/O with page cache in SimGrid.
		\end{itemize}
	\section{Related Work}			
		
		\subsection{Page cache}
			\begin{itemize}
				\item What is page cache? How it works \cite{linuxdev3rd2010}. Effects and importance of page cache.
				\item Introduce some existing strategies with some highlighted pros and cons.
				\item Current implementation in Linux and some reasons why it is chosen to be implemented (implementation complexity, effectiveness, overhead, etc) \cite{linuxdev3rd2010}
			\end{itemize}									

		\subsection{Simulators}
			\begin{itemize}
				\item Discuss some existing methods, simulation frameworks to conduct HPC experiments. Compare pros and cons (accuracy, simulation time, usability) of some simulators (SimGrid, GridSim).
				\item Related development: RAM energy consumption \cite{gill2019} \cite{ouarnoughi2017} 
				\item Discuss the pros of SimGrid and the reasons why we chose it to extend. (Section 2.2.2 in \cite{casanova2014})
			\end{itemize}
			
	\section{Method}
		In this section, we present our approach to model file read/write, 
		I/O in memory, data flushing and cache eviction mechanisms 
		implemented in Linux. 
		We also detail the design the simulators in order to simulate I/O activities 
		and the implementation in Python and SimGrid framework. 
		Finally, we describe some experimental scenarios to evaluate our model 
		with a Python simulator, a baseline SimGrid simulator, a simulator 
		with SimGrid integrated with our model, and the results from real pipelines.

		\subsection{Principle of the simulator}
	
			In our I/O simulation model, we try to simulate the write back page cache 
			mechanisms including memory I/O, cache eviction and data flushing as we 
			believe that they are the most important factors that affect the I/O time. 
			We separate our model into components and encapsulate them in a 
			kernel-like layer described in Figure~\ref{fig:interaction}. 
			This layer consists of two main classes, MemoryManager and IOManager.
			MemoryManager is responsible for memory simulation, which includes 
			memory I/O, flushing, cache eviction, and exposed its functions 
			to IOManager. The IOController class models higher abstraction with the 
			simulation of file read and file write.
			With this layer, simulated applications can simply request for 
			file reads and file writes by calling functions of IOManager class 
			instead of calling directly to storage simulation models.

			\begin{figure}
   				\centering
   				\includegraphics[width=0.85\columnwidth]{figures/interaction.pdf}
   				\caption{The additional layer between user application layer and 
   				storage simulation models}\label{fig:interaction}
			\end{figure}	

			\subsubsection{Memory simulation model}

			We model memory with capacity, read bandwidth and write bandwidth. 
			When data is read from or written to memory, the actual 
			read/write time is calculated with the amount of data and memory 
			read/write bandwidth:
			
			\begin{align*}
				& t_{mr} = D / r_m \\ 
				& t_{mw} = D / w_m\
			\end{align*}		
			
			where:
			\begin{itemize}
				\item $t_{mr}$ is time to read data from memory
				\item $t_{mw}$ is time to write data to memory
				\item $D$ is the amount of data to read/write
				\item $r_m$ is memory read bandwidth
				\item $w_m$ is memory write bandwidth
			\end{itemize}						
			
			We introduce the concept of data block as a unit to represent cached data. 
			A data block is a subset of pages of a file in page cache that is accessed 
			at a specific time. Therefore, a block has the information about file name, 
			block size, last access and a dirty flag with values of 0 and 1, 
			0 represents block of lean data, while 1 indicates that the data block 
			is dirty. 
			Cached data of a file can be represented by multiple blocks. 
			Because a data block represents a subset of cached pages, 
			it can be split into smaller blocks, and multiple blocks can also 
			be merged into a larger one.
			
			In Linux kernel, the page cache LRU lists are lists of pages of 
			files in cache. In our simulator, instead of using lists of pages, 
			we simulate these lists with two lists of data blocks, an active list and 
			an inactive list, ordered by block last access as is shown in 
			Figure~\ref{fig:lrulist}.
			As in the kernel, in our simulator, we keep the size of the active list not 
			larger than twice the size of the inactive list by moving least recently 
			used blocks from the active list to the inactive list. 
			
			\begin{figure}
   				\centering
   				\includegraphics[width=\columnwidth]{figures/lru_lists.pdf}
   				\caption{Simulation of page cache LRU lists with data blocks}\label{fig:lrulist}
			\end{figure}	
			
			Modeling page cache LRU lists as lists of blocks offers our simulator 
			some obvious advantages. 
			Maintaining and handling lists of pages in the simulator, which is in 
			packet level, are non-trivial when they require significant implementation 
			overhead and would likely to hinder the performance of the simulator. 
			Therefore, using page cache lists of data blocks can simplify the 
			simulation of cache eviction and data flushing while preserving
			the accuracy in i/o time simulation. 
			
			With the simulation model of page cache LRU lists of blocks, now we can 
			model I/O in memory.
			At the time when a file is accessed, some data of the file can be in 
			cache or not. 
			If accessed data is already cached, there are existing data blocks 
			representing cached data in two page cache LRU lists. 
			As cached data is accessed again, these blocks are moved to 
			the active list, dirty blocks are merged into a bigger dirty block, 
			while clean data blocks are merged into another clean block. 
			If accessed data is not in cache, new blocks representing uncached data 
			are created and put into the top of the inactive list. 
			For more detail when uncached data is accessed, in the case of 
			reading file, as all uncached data is clean data, only one clean data 
			block is added to the inactive list.
			But in the case of writing uncached file data though cache, 
			this written data can be either dirty or clean. 
			Thus, at most two data blocks can be added to the inactive lists, 
			one is clean and one is dirty. 
			We also keep the size of the active list not larger than twice the size of 
			the inactive list as in the kernel. This is done by moving least recently 
			used blocks in the active list to the inactive list when the active list 
			grows. If only a part of a data block needs to be moved, 
			the block is split.

			Next, we simulate data flushing mechanism, which writes back 
			dirty data to disk. 
			The data flushing simulation function traverses the sorted inactive first, 
			and then the active list, looks for least recently used dirty blocks and 
			sets dirty flags of these blocks to 0 until the total flushed amount 
			reaches the amount to be flushed or there is no dirty data left in cache. 
			If the last flushed block is not entirely flushed, it is split into 
			two blocks, one is flushed and one remains dirty.
			The flushing time is calculated as:
					
			\begin{align*}
				t_f = A / w_d
			\end{align*}		
			
			where:
			\begin{itemize}
				\item $t_f$ is the flushing time
				\item $A$ is the amount of data to be flushed
				\item $w_d$ is the disk write bandwidth
			\end{itemize}		
			
			For periodical flushing, our simulator searches in two page cache 
			LRU lists and flushes all the expired dirty data block. 
			A dirty data block is considered expired if the time from its last access 
			to current simulated time is longer than the predefined expire time. 
			Then the flushing time is calculated with total amount of flushed data.
				
			Similarly to data flushing, our cache eviction simulation function frees up 
			the page cache by traversing and deleting least recently used clean 
			data blocks in the inactive list.
			The amount of evicted data is passed in and data blocks are deleted 
			from the inactive list until total evicted data reaches the amount 
			passed in, or there is no clean block left in the list.
			If the last evicted block is not entirely evicted, the block is split, 
			and only one block is deleted.
			Our cache eviction simulation does not add up to the simulated time 
			since cache eviction time is negligible in real systems.			
			
			\subsubsection{File I/O simulation model}			
			
			Simulated applications can simply request for a file read 
			or file write by calling the APIs of this layer instead of 
			calling directly to storage simulation models.
			Because every computer has its own memory and system configurations, 
			one IOController object and one MemoryManager object are created 
			for each simulated machine.
			
			To read or write a file, a simulated application sends a request to the 
			IOController instance on the node where the requested file is stored.
			Based on the memory status in MemoryManager, the IOController 
			orchestrates cache read/write, disk read/write, flushing and cache eviction 
			to fulfill I/O requests, calculates total simulated time and returns to 
			simulated applications.
			
			\begin{algorithm}\caption{File read simulation}\label{alg:read}
				\small
				\begin{algorithmic}[1]
					\Input
        				\Desc{fn}{file name}
        				\Desc{fs}{file size (assumed to fit in memory)}
						\Desc{rt}{current simulated run time}
						\Desc{mm}{MemoryManager object}
						\Desc{rm}{memory read bandwidth}
						\Desc{sm}{storage simulation model}
   					\EndInput
   					\Output
						\Desc{rt}{simulated run time after the file is read}
   					\EndOutput
					\State rt = rt + mm.flush(2*fs - mm.cached(fn) - mm.evictable\_mem) 
					\State mm.evict(2*fs - mm.cached(fn) - mm.free\_mem) 
					\If {mm.cached(fn) $>$ 0}  \Comment{Read file cached data}
    					\State mm.cache\_read(fn)  \Comment{Update blocks in LRU lists}
    					\State pft = mm.periodic\_flush(rt) \Comment{Periodical flushing time}
						\State rt = rt + max(mm.cached(fn) / rm, pft) \Comment{Add the longer}
					\EndIf
					\If {mm.cached(fn) $<$ fs} \Comment{Read uncached data from disk}
						\State rt = rt + mm.periodic\_flush(rt) \Comment{Another flushing interval}
						\State mm.disk\_read(fs - mm.cached(fn), fn)
    					\State rt = rt + sm.read(fs - mm.cached(fn))
					\EndIf					
					
					\State \Return rt
					
				\end{algorithmic}
			\end{algorithm}			
			
			The algorithm used to simulate file read is described in 
			Algorithm~\ref{alg:read}. 
			In Linux systems with write back storage devices, data can be either read 
			from cache with memory bandwidth or read from disk with disk bandwidth. 
			Therefore, total read time can be considered as a linear function 
			of the read time with each of these bandwidths, 
			enabling us to generalize reading process as two separate phases, 
			cache read and disk read. 
			In our simulator, when a user simulated application reads a file, 
			it sends a file read request to IOController. 
			First, dirty data is flushed to disk if needed to make the amount of 
			available memory enough to read the file.
			Data flushing function in MemoryManager is called, flushing time is 
			simulated and added to total run time (line 10).
			After data is flushed, MemoryManager is asked to evict data 
			from cache to accommodate the requested file (line 11). 
			Next, if file is cached either partially or entirely, MemoryManager is 
			called to re-access and update this cached data (line 13). 
			Then, periodical flushing is called to flushed expired dirty data 
			from cache (line 14).
			Since periodical flushing and cache read can be concurrent, the longer 
			time is added to simulated run time (line 15).
			If file is not cached or partially cached, a disk read is required (line 17). 
			Periodical flushing is called once again, but this time the flushing time is 
			added to total simulated time since flushing and disk read are time-shared, 
			or sequential in other words (line 18).
			Finally, data is read from disk to cache and managed by MemoryManager, 
			disk read time estimated by the storage model is added to 
			total simulated run time (line 19-20). 

			\begin{algorithm}\caption{File write simulation}\label{alg:write}
				\small
				\begin{algorithmic}[1]
					\Input
        				\Desc{fn}{file name}
        				\Desc{fs}{file size}
						\Desc{rt}{current simulated run time}
						\Desc{mm}{MemoryManager object}
						\Desc{wm}{memory write bandwidth}
						\Desc{sm}{storage simulation model}
   					\EndInput
   					\Output
						\Desc{rt}{simulated run time after the file is read}
   					\EndOutput
   					\State pft = mm.periodic\_flush(rt)
					\State remain\_dirty = dirty\_ratio * mm.avail\_mem - mm.dirty
					\State mem\_amt = min(fs, remain\_dirty)
					\If {remain\_dirty $>$ 0} \Comment{Write with memory bandwidth}
    					\State mm.evict(mem\_amt - mm.free\_mem)
    					\State mm.write(fn, mem\_amt)
    				\EndIf
    				\State rt = rt + max(mem\_amt / wm, pft) \Comment{Concurrent processes}
					\If {mem\_amt $<$ fs}  \Comment{Write with disk bandwidth}
						\State mm.flushed(fs - mem\_amt)  \Comment{Concurrent flushing}
						\State mm.evict(fs - mem\_amt  - mm.free\_mem) 
						\State mm.write(fn, min(fs - mem\_amt, mm.free\_mem))
						\State rt = rt + sm.write(fs - mem\_amt)
					\EndIf
					
					\Return rt
					
				\end{algorithmic}
			\end{algorithm}

			Algorithm~\ref{alg:write} describes our simulation of file write. 
			Initially, periodical flushing is called to flush expired dirty data 
			at the beginning of the write (line 10).
			Similar to the file read simulation, a file can also be written with 
			two different bandwidths, memory bandwidth and disk bandwidth. 
			Files can be written with memory bandwidth before the amount 
			of dirty data reaches dirty\_ratio. Thus, our algorithm initially checks 
			the amount of data that can be written with memory write 
			bandwidth (line 11-12).
			If this amount is greater than 0, MemoryManager is asked to evict 
			data from cache if needed and write this amount of data to cache 
			(line 14-15).
			The simulated run time can be added with the longer time of cache write 
			time and periodical flushing time because these can happen 
			concurrently (line 17).
			The amount of data that is not written with memory bandwidth is written 
			with disk bandwidth (line 18). 
			Now, the dirty ratio threshold is reached, we have to flush and evict 
			data from cache as much as possible so that 
			the remaining data can be written to cache (line 19-20). 
			Here, MemoryManager does not guarantee that the amount of flushed 
			and evicted data is enough for the remaining data. In the case it is 
			less than the required amount, the remaining data is written to cache 
			but then can be flushed and evicted right away. Thus, the amount 
			of remaining data that is written and kept in cache after the write 
			is limited by the amount free memory (line 21). 
			Finally, the write time is simulated, added to the total time and 
			returned to simulated applications (line 22).
			
		\subsection{Implementation}

			To validate our simulation model, we create a simple 
			simulator which is independent to simulation frameworks and libraries. 
			This enables us to evaluate the accuracy and correctness of our 
			model in a simple use case before integrating it to SimGrid. 
			This simulator simulates a single-threaded pipeline running on a system 
			with a single core CPU, one local storage device and all input and 
			output files are stored locally. The results from the simulator is compared 
			and validated with the results from a real pipeline running on
			a real system.
			
			Having our model validated, we create simulators for different use cases 
			using the current version of SimGrid and the SimGrid version that is 
			extended with our model. We leverage Wrench framework, which is 
			built on top of SimGrid to simulate our pipelines and system.
			Then, we compare the results of the simulators with
			original SimGrid and extended SimGrid with the results of real
			pipelines on a real system. 
		
			In this work, we use Python 3.7 to implement the simple
			simulator, SimGrid 3.25 and Wrench 1.6 for SimGrid simulators. 
			SimGrid source code is available at 
			\url{https://framagit.org/simgrid/simgrid}, and Wrench is provided on
			github at \url{https://github.com/wrench-project/wrench}.
			
		\subsection{Experiments}
		
			When designing experiments, our goal is to evaluate our simulation model 
			in single-threaded and multi-threaded applications which access data on 
			on different file system types. 
			To do that, we create a pipeline of sequential tasks, each task 
			reads an input file, increases every byte of the input, and writes the 
			output to disk. The output of the previous task is the input of 
			the next task. We also adopt an use case in real applications and 
			to evaluate the applicability of our simulation model.
			
			The simplest scenario is a single-threaded application, where memory 
			bandwidth and disk bandwidth are not shared with another thread. 
			This allows us to compare the detailed results in terms of read/write time, 
			memory states after every single task and ,more importantly, validate our 
			simulation model. 
			
			For multi-threaded applications, we do the experiment with a varying 
			number of concurrent pipelines running on a multi-core host 
			and writing to the same local storage device. 
		    By doing so, we can assess the performance of our model in the case  
		    of concurrent I/O with shared bandwidth on a local storage device.
		    
		    Since different storage types should also be taken into account, 
		    we conduct the next experiment with the same scenario as 
		    in the multi-threaded experiment, except that files are stored on a 
		    shared file system instead of local disk. 
		    In this way, we can not only evaluate our simulation model on I/O 
		    with shared file system, but also compare the 
		    performance of the simulator on different storage types. 
		    
		    Finally, we adopt a real pipeline in a real neuroimaging applications. 
		    We simulate the pipeline with a simulated workflow in SimGrid 
			and compare the simulation results with the real results. 
			Because our work focuses on I/O time, we assume that CPU time is 
			correctly modeled and use the CPU time measured in the real pipelines 
			to setup our simulated workflows. 
			
			We use the cluster of \textit{Big Data Infrastructure for Neuroimaging Lab} 
			at \textit{Concordia University} to conduct our experiments. The cluster 
			consists of one login node which is the control node, 8 compute nodes 
			and 4 storage nodes connected with two network switches. The login node 
			has one Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz, 128GiB of RAM, 1.8TB 
			of storage of XFS file system, 13GB of tmpfs file system and 385 TB of 
			Lustre shared file system. Each compute node has two 16 cores Intel(R) 
			Xeon(R) Gold 6130 CPU @ 2.10GHz, 256 GB of RAM and 6 SSDs, 450GB each 
			with XFS file system, 378GB of tmpfs and 126GB of devtmpfs file system.
			Lustre file system is configured with one metadata target with 854GB 
			of storage, 44 object storage targets with 8.7TB of storage each. 
			The cluster is run on CentOS 8.1 with the Slurm Workload Manager installed. We use 
			\textbf{\textit{atop}} and \textbf{\textit{collectl}} as tools to monitor 
			and collect data of memory, page cache status and disk throughput on 
			the cluster. The cluster nodes are connected with ... 
			\textcolor{red}{[network description here]}
	
			\subsubsection{Single-threaded evaluation}

				In this experiment, our goal is to validate our model by comparing 
				the simulation results of the simple Python simulator with results of 
				a simulator implemented with original SimGrid, a simulator 
				implemented with SimGrid integrated with our model, and a real pipeline 
				running of the cluster. 
				The pipeline consists of three tasks, each task reads a file, 
				increases every byte and writes output file to local disk. 
				We use the input sizes of 20 GB, 50 GB, 75 GB and 100 GB, run 
				the pipeline on the cluster, measure the CPU time of the pipeline 
				with each input to simulate the pipeline with the simulators.

			\subsubsection{Multi-threaded evaluation}

				In the second experiment, we evaluate the simulator in concurrent I/O 
				with a multi-threaded application. As we run the pipelines on 
				only one compute node of the cluster, which has upto 32 cores per node,  
				we create 32 input files with the size of 3GB each and vary the number of 
				concurrent pipelines from 1 to 32. 
				Because multi-threaded simulator implementation in Python is 
				expensive, and we have our model integrated in SimGrid, 
				we only compare the results of simulators with original SimGrid, 
				SimGrid integrated with our model and and real pipelines. 
				The results are compared in total makespan of the pipelines, 
				cumulative read time and cumulative write time.
			
			\subsubsection{Evaluation on storage types}

				

			\subsubsection{Simulation of a real application}
				A real pipeline (for example a pipeline with nighres)

	\section{Results}
	
		\begin{itemize}

			\item Quantitative results: 
				\begin{itemize}
					\item Errors of simulation time and memory used compared to real results.
					\item Simulation time compared to baseline SimGrid.
				\end{itemize} 

			\item Ability of the model to generalize memory trends (dirty data, cache used) and disk throughput.

		\end{itemize}

	\section{Discussion and Future Work}
		\begin{itemize}
			\item Sensitivity of the simulator on the variation of memory and disk bandwidth. 
		\end{itemize}
\bibliographystyle{plain}
\bibliography{citation}

\end{document}