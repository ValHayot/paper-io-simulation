\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}
\usepackage{hyperref}

\algblock{Input}{EndInput}
\algnotext{EndInput}
\algblock{Output}{EndOutput}
\algnotext{EndOutput}
\newcommand{\Desc}[2]{\State \makebox[2em][l]{#1}#2}

\newcommand{\tristan}[1]{\color{orange}\textbf{From Tristan:}#1\color{black}}
\newcommand{\valerie}[1]{\color{blue}\textbf{From Valerie:}#1\color{black}}


\begin{document}
\title{I/O simulation model with Linux page cache, integration and evaluation in SimGrid framework}

\author{\IEEEauthorblockN{Hoang-Dung Do, Tristan Glatard, Val\'erie Hayot-Sasson
  }\\
  \IEEEauthorblockA{
    Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada
  }
}

\maketitle

	\begin{abstract}
		\begin{itemize}
			\item The I/O bottleneck in HPC and the need of experiments.
			\item HPC experiment frameworks and advantages of SimGrid.
			\item The missing of the ability to simulate page cache, the goal of this paper.
			\item Principle of the simulator, experiment scenarios and comparisons.
			\item Brief discussion on results and future work.
		\end{itemize}
	\end{abstract}

	\section{Introduction}
		\begin{itemize}
			\item HPC, the bottleneck in I/O and the demand of HPC experiments. 
			\item Difficulties in conducting high performance computing experiments and the need of simulation frameworks.
			\item Existing experiment methods, simulators, simulation frameworks. The advantages of SimGrid compared to others \cite{simgrid2008, lebre2015}. The missing of the ability to simulate page cache in SimGrid \cite{lebre2015}.
			\item The objective of the paper: Add capability to simulate I/O with page cache in SimGrid.
			\item Clarify, distinguish between memory I/O (i/o to page cache) and disk I/O.
		\end{itemize}
	\section{Related Work}			
		
		\subsection{Page cache}
			\begin{itemize}
				\item What is page cache? How it works \cite{linuxdev3rd2010}. 
				Write-back vs writethrough and direct i/o. Effects and importance of page cache.
				\item Introduce some existing strategies with some highlighted pros and cons.
				\item Current implementation in Linux and some reasons why it is chosen to be implemented (implementation complexity, effectiveness, overhead, etc) \cite{linuxdev3rd2010}
				\item \tristan{LRU lists are mentioned in the next section, they should be described here.}
			\end{itemize}									

		\subsection{Simulators}
			\begin{itemize}
				\item Discuss some existing methods, simulation frameworks to conduct HPC experiments. Compare pros and cons (accuracy, simulation time, usability) of some simulators (SimGrid, GridSim).
				\item Related development: RAM energy consumption \cite{gill2019} \cite{ouarnoughi2017} 
				\item What is implemented in simulators that useful for our model:
				distinguished and shared bandwidths, concurrency, device latency. 
				\item Discuss the pros of SimGrid and the reasons why we chose it to conduct experiments. (Section 2.2.2 in \cite{simgrid2014})
				\item Introduce WRENCH and it's advantage.
				\item \tristan{Mention the status of storage simulation in simulators, so that we understand from Fig 1 that storage devices are available}
			\end{itemize}
			
	\section{Method}

		In this section, we present our approach to model file read and write with
		I/O to page cache, cache eviction and data flushing mechanisms implemented 
		in the Linux kernel. We also detail the design of our simulators and their
		implementation. Finally, we describe experimental scenarios to evaluate
		our simulation model on real applications using a standalone Python
		prototype, SimGrid framework using original WRENCH and the WRENCH version 
		that is integrated with our simulation model (WRENCH-Ext).

		\subsection{Principle of the simulator}
	
            We separate our simulation model in two components, the IO
			Controller and the Memory Manager, which together simulate 
			file read/write mechanisms in Linux (Figure~\ref{fig:interaction}). 
			The Memory Manager simulates I/Os to cache, flushing, periodical flushing, 
			and cache eviction, whereas the IO Controller simulates file reads and writes. 
			Simulated applications can simply interact with the IO Controller instead of 
			calling simulated storage directly as is the case with current simulators.

			\begin{figure}
   				\centering
   				\includegraphics[width=0.85\columnwidth]{figures/interaction.pdf}
   				\caption{Overview of the simulated write-back page cache}\label{fig:interaction}
			\end{figure}	

			\subsubsection{Memory simulation model}
			
			We leverage existing storage simulation models to simulate the memory,
			which is characterized with storage capacity, distinguished read/write 
			bandwidths and device latency. 
			All of these features are dealt within models in simulation 
			frameworks, which makes storage models applicable to memory \cite{lebre2015}. 
			In addition, leveraging existing disk models allows us 
			to take advantage of bandwidth sharing between concurrent disk I/O to 
			simulate parallel memory accesses.\tristan{we should refer to related work, 
            see comment there} \valerie{Agreed. It's unclear how existing storage models enable this at the moment}.

			Memory Manager simulates two parallel threads. The first thread is the 
			main thread where activities of applications are simulated, and responsible 
			for the simulation of data flushing, cache eviction and I/Os to cache. 
			The second thread is a simulated background thread, in which Memory Manager 
			periodically searches and flushes expired dirty data in page cache LRU lists 
			to disks. 

			In Linux kernel, the page cache LRU lists contain file pages. However, 
			due to the large number of file pages, managing lists of pages may 
			require significant computation overhead. 
			Thus, we introduce the concept of a data block as a unit to represent data 
			cached in memory. A data block is a subset of file pages stored in
            page cache that were accessed in the same I/O operation. 
			A data block has information about file name, block size, last access 
			time, a dirty flag that represents whether the data is clean (0) 
			or dirty (1), and the time it is written to cache as dirty data. 
			Blocks can have different sizes and a given file can have multiple 
			data blocks in page cache. In addition, a data block can be split into an 
			arbitrary number of smaller blocks.

			\begin{figure}
   				\centering
   				\includegraphics[width=\columnwidth]{figures/lru_lists.pdf}
   				\caption{Simulation of page cache LRU lists with data blocks}	\label{fig:lrulist}
			\end{figure}	
			
			Now, page cache LRU lists can be modeled with two lists of data blocks: 
			an active list and an inactive list, ordered by last access time 
			(earliest first, see Figure~\ref{fig:lrulist}).
			As in the kernel, our simulator keeps the size of the active list under
			twice the size of the inactive list by moving least recently 
            used data blocks from the active list to the inactive list.
			Modeling page cache LRU lists as lists of data blocks reduces the
			overhead of the simulator, as data blocks are obviously less
			numerous than file pages, while achieving the accuracy in I/O time
			simulation that can be seen in the later section.
			
			Given an arbitrary file, at a specific time, it can be partially cached, 
			completely cached, or uncached.
			Cached data is composed of data blocks existing in either or both
            of the page cache LRU lists. Cached blocks reside in the inactive list
            unless they have been recently reaccessed. Cached blocks 
            located on the inactive list that are subsequently accessed, 
            will be moved to the top of the active list. Cached blocks 
            recently written to cache are dirty blocks until they are flushed to disk.

            Our model supports chunk-by-chunk file read/write simulation
            with an user-defined chunk size, in which chunks are read/written 
            until file is entirely read/written. 
            The simulated LRU lists are used as follows. 
            When a file is read, its data can be partially cached, completely cached, 
            or uncached. 
            Uncached data is read from disk first, and cached data is only read 
            after all uncached data is read (left to right in Figure~\ref{fig:read_order}).
            When a chunk of uncached data is read, a new clean block is created 
            and put to the top of the inactive list. 
            For a chunk of cached data, because one or more existing data blocks 
            in two page cache LRU lists are accessed again, read dirty blocks are 
            moved to the active list, while clean data blocks are merged into 
            a single clean block and then also put in the active list. 
			The reason we do not merge dirty blocks is that different blocks 
			are written to cache at different time, and this time is used later for 
			periodical flushing process.
			In case a block is not entirely read, it is split into two smaller blocks, 
			one is reaccessed and the other remains the same.
			
			\begin{figure}
   				\centering
   				\includegraphics[width=\columnwidth]{figures/read_order.pdf}
   				\caption{File data read order}	\label{fig:read_order}
			\end{figure}	
			
			In the case of writing data, we can assume that all data to be written is 
			uncached and it is written to cache as dirty data. 
			Thus, each time a chunk is written, we create a block of dirty data and 
			put it in the inactive list.
			
			\textcolor{red}{[Is there any kind of accesses other than read and write?]}. 			
			
            As is done in the kernel, we ensure the size of the active list does not 
            exceed twice the size of the inactive list 
			\cite{gorman2004understanding, linuxdev3rd2010}. 
			This is done by moving least recently used blocks in the active list 
			to the inactive list when the active list grows. If only a part of a data 
			block needs to be moved, the block is split.

			Next, we simulate data flushing, which runs on the main simulated 
			thread and writes back dirty data to disk. 
			The data flushing simulation function traverses the sorted 
			inactive list first, and then the active list, looks for least recently used 
			dirty blocks, writes them to disk through the storage device model, 
			and sets dirty flags of these blocks to 0 until the total flushed amount 
			reaches the amount to be flushed or there is no dirty data left in cache. 
			If the last flushed block is not entirely flushed, it is split into 
			two blocks, one that is flushed and one that remains dirty.
			The flushing time is simulated with storage model.
				
			Similarly to data flushing, our cache eviction simulation runs on 
			the main thread, frees up the page cache by traversing and deleting 
			least recently used clean data blocks in the inactive list.
			The amount of evicted data is passed in and data blocks are deleted 
			from the inactive list until total evicted data reaches the amount 
			passed in, or there is no clean block left in the list.
			If the last evicted block is not entirely evicted, the block is split, 
			and only one block is deleted.
			The cache eviction simulation does not add up to the simulated time 
			since cache eviction time is negligible in real systems.		
			
			\begin{algorithm}\caption{Periodical flushing simulation}\label{alg:pdflush}
				\small
				\begin{algorithmic}[1]
					\Input
        				\Desc{in}{page cache inactive list}
        				\Desc{ac}{page cache active list}
						\Desc{t}{predefined flushing time interval}
						\Desc{sm}{storage simulation model}
   					\EndInput
   					\While{host is on}
						\State blocks = expired dirty blocks in active and inactive list
						\State flushing\_time = sm.write(blocks)
						\State Flag blocks as clean data
						\If{flushing\_time $<$ t}
							\State sleep(t - flushing\_time)
						\EndIf
					\EndWhile
				\end{algorithmic}
			\end{algorithm}				
			
			The periodical flushing simulation is run on a simulated background thread 
			to flush expired dirty data blocks from cache to disk.
			Similar to expired dirty pages in real Linux systems, a dirty block in our model 
			is considered expired if the time it remains in cache as dirty is longer than 
			the predefined expire time. This time is counted from the time it is written 
			to cache to current simulated time. 
			The simulation algorithm of periodical flushing is an infinite loop in which 
			Memory Manager searches and flushes these expired dirty blocks to disk, 
			as described in Algorithm~\ref{alg:pdflush}. 
			The loop ends when the host the host is off (line 6). 
			In each repetition, Memory Manager finds expired dirty blocks in two 
			page cache LRU lists (line 7), simulates writes of data of these blocks 
			to disk (line 8), and mark them as clean (line 9).
			If the flushing time does not exceed our time interval, the thread is put 
			to sleep for the remaining time (lines 10-12). 
			Then based on the state of the host at current simulated time, 
			the algorithm continues or finishes the loop .
			Because periodical flushing is simulated as a background thread, it can be 
			concurrent with disk I/O on main thread. This parallelism is handled by the 
			storage model and reflected in simulated I/O time. 	

			\subsubsection{File I/O simulation model}			
			 
			When simulated applications need to read/write a file, they can simply call 
			the IO Controller instead of the storage simulation models. 
			\tristan{this should move, not sure where for now: 
			Because every computer has its own memory and system configurations, 
			one IO Controller object and one Memory Manager object are created 
			for each simulated host.}
			
			As mentioned, in our model, a file read/write can be complete by read/write 
			chunk-by-chunk until entire file is read/written.
			To read or write a chunk of file, a simulated application sends a request to the 
			IOController instance on the node where the requested file is stored.
			Based on the memory status in Memory Manager, the IOController 
			orchestrates cache and disk reads/writes, flushing, cache eviction 
			to simulate I/O time.
			
			\begin{algorithm}\caption{File chunk read simulation}\label{alg:read}
				\small
				\begin{algorithmic}[1]
					\Input
        				\Desc{cs}{chunk size}
        				\Desc{fn}{file name}
        				\Desc{fs}{file size (assume to fit in memory)}
						\Desc{mm}{MemoryManager object}
						\Desc{sm}{storage simulation model}
   					\EndInput
   					\State disk\_read = min(cs, fs - mm.cached(fn))
   					\State cache\_read = cs - disk\_read
					\State mm.flush(cs + disk\_read - mm.free\_mem - mm.evictable, fn) 
					\State mm.evict(cs + disk\_read - mm.free\_mem, fn) 
					\If {disk\_read $>$ 0}  \Comment{Read uncached data}	
						\State sm.read(disk\_read)  
						\State mm.add\_to\_cache(disk\_read, fn) 	
					\EndIf
					\If {cache\_read $>$ 0} \Comment{Read cached}
						\State mm.cache\_read(cache\_read)  
					\EndIf
				\end{algorithmic}
			\end{algorithm}			
			
			The algorithm to simulate a chunk read is described in Algorithm~\ref{alg:read}.   
			When a simulated application read a chunk of a file, it sends a 
			chunk read request to IO Controller. 
			First, we calculate the amount of uncached data that needs to be read 
			from disk, and the remaining amount is read from cache (line 7-8).
			We assume that after a file is read, there are two copies of the file in memory 
			(one in cache, one in anonymous memory). 
			Thus, when a chunk is read, one copy of the chunk occupies anonymous memory, 
			and the uncached part of that chunk is read into cache.	
			The amount of required memory to read the chunk is calculate and 
			if there is not enough memory available, Memory Manager is called to 
			flush dirty data to disk and simulate flushing time with memory model (line 9).
			This flushing is complemented by eviction in the Memory Manager (line 10). 
			Then, if there is uncached data, a disk read is simulated with disk model (line 12), 
			and this data of the file is added to cache (line 13)
			Finally, if cached data is read, Memory Manager is called to simulate a cache read, 
			update the corresponding data blocks in cache (line 16). 

			\begin{algorithm}\caption{File chunk write simulation}\label{alg:write}
				\small
				\begin{algorithmic}[1]
					\Input
        				\Desc{cs}{chunk size}
        				\Desc{fn}{file name}
						\Desc{mm}{MemoryManager object}
						\Desc{sm}{storage simulation model}
   					\EndInput
					\State remain\_dirty = dirty\_ratio * mm.avail\_mem - mm.dirty
					\If {remain\_dirty $>$ 0} \Comment{Write with memory bandwidth}
    					\State mm.evict(min(cs, remain\_dirty) - mm.free\_mem)
    					\State mem\_amt = min(cs, mm.free\_mem)
    					\State mm.write(fn, mem\_amt) 
    				\EndIf
					\If {mem\_amt $<$ cs}  \Comment{Write with disk bandwidth}
						\State mm.flush(cs - mem\_amt)  
						\State mm.evict(cs - mem\_amt  - mm.free\_mem) 
						\State mm.add\_to\_cache(fn, min(cs - mem\_amt, mm.free\_mem))
						\State sm.write(cs - mem\_amt)
					\EndIf
					
				\end{algorithmic}
			\end{algorithm}

			Algorithm~\ref{alg:write} describes our simulation of chunk write. 
			In our chunk write simulation, data can be written with either memory bandwidth 
			or disk bandwidth. 
			Data can be written to cache with memory bandwidth without waiting for 
			flushing or being throttled before the amount of dirty data reaches dirty\_ratio. 
			Thus, our algorithm initially checks the remaining amount of data that can be written 
			as dirty data (line 6).
			If this amount is greater than 0, MemoryManager is asked to evict 
			data from cache to accommodate the written data if needed (line 8). 
			After evicting cache, the actual amount of data that can be written with 
			memory bandwidth is calculated (line 9), a cache write is then simulated 
			with this amount through memory model and data written is added to 
			page cache LRU lists (10).
			After dirty\_ratio is reached, dirty data has to be flushed before new data 
			can be written to cache again. 
			If there is remaining chunk data that has not been written, we flush and 
			evict data from cache as much as possible so that the this data can be 
			written to cache (line 13-14). 
			Here, MemoryManager does not guarantee that the amount of flushed 
			and evicted data is enough for the write of remaining data. 
			In the case it is less than the required amount, the remaining data 
			being written to cache can be flushed and evicted right away. 
			Thus, the amount of remaining data written and kept in cache 
			after the write is limited by the amount free memory, and this amount 
			is added to cache (line 15). 
			Finally, the disk write is simulated with the remaining data amount (line 16), 
			and the chunk write algorithm finishes.
			
		\subsection{Implementation}

			To validate our simulation model, we create a simple prototype
			simulator independent of existing simulation frameworks and libraries. 
			This enables us to evaluate the accuracy and correctness of our 
			model in a simple scenario before integrating it to the more complex WRENCH framework. 
			In this prototype we use the following basic storage model, used for both memory and disk: 
			\begin{align*}
				& t_{r} = D / b_r \\ 
				& t_{w} = D / b_w\
			\end{align*}		
			
			where:
			\begin{itemize}
				\item $t_{r}$ is the data read time
				\item $t_{w}$ is the data write time
				\item $D$ is the amount of data to read/write
				\item $b_r$ is the read bandwidth of the device
				\item $b_w$ is the write bandwidth of the device
			\end{itemize}			

			Since bandwidth sharing is not simulated, this prototype doesn't support 
			concurrency: it is limited to single-threaded pipelines running on systems 
			with a single-core CPU. We use it in a first validation of our simulation 
			model, against a real sequential pipeline running on a real system.
			Source code of our Python prototype is available at 
            \url{https://github.com/big-data-lab-team/paper-io-simulation/tree/master/exp/pysim}
			
			Having our model validated, we create simulators for different use cases 
			using WRENCH.
			We integrate our I/O model to the original WRENCH by implementing 
			the core of the model separately from existing WRENCH components, 
			then providing an API for users to call to our simulation model. 
			We also add an optional command line argument so that users will be able 
			to choose use or not to use the model. Doing this, we can avoid 
			unexpected impacts on original WRENCH and can independently update 
			the model in the future. 
			We then compare the results of the simulators with
			original WRENCH and extended WRENCH with the results of real
			pipelines on a real system. 
		
			In this work, we use Python 3.7 to implement the single-threaded simulator, 
			SimGrid 3.25 and WRENCH 1.6 for more multi-threaded simulators. 
			SimGrid source code is available at \url{https://framagit.org/simgrid/simgrid}, 
			and WRENCH is at \url{https://github.com/wrench-project/wrench}.
			
		\subsection{Experiments}
		
        	The goal of our experiments is to evaluate our write-back cache 
			simulation in single-threaded and multi-threaded applications
			accessing data on different types of file system. We use a pipeline
			of sequential tasks where each task reads an input file, increments
			every byte of the input to emulate real processing and avoid caching
			effects, and writes the result to disk. The output of the previous
			task is the input of the next task. We also use a real application 
            pipeline \textcolor{red}{[Maybe briefly describe this application's purpose (e.g. a standard fMRI preprocessing piepeline)]}, to evaluate the applicability of our simulation model. 
			
			We use our dedicated cluster hosted at Concordia University to conduct 
			the experiments. The cluster consists of one login node, 8 compute nodes 
			and 4 storage nodes connected with two network switches. The login node 
			has one Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz, 137~GB (128~GiB) of RAM, 
			1.8TB of storage of XFS file system, 13~GB of tmpfs file system and 385 TB of 
			Lustre shared file system. Each compute node has two 16 cores Intel(R) 
			Xeon(R) Gold 6130 CPU @ 2.10GHz, 275~GB (256~GiB) of RAM and 6 SSDs, 
			450~GB each with XFS file system, 378~GB of tmpfs and 126~GB of devtmpfs file system.
			Lustre file system is configured with one metadata target with 854~GB 
			of storage, 44 object storage targets with 8.7~TB of storage each. 
			The cluster is run on CentOS 8.1 with the Slurm Workload Manager installed. 
			We use \textbf{\textit{atop}} and \textbf{\textit{collectl}} as tools to 
			monitor and collect data of memory, page cache status and disk throughput 
			on the cluster. 
			\textcolor{red}{[Add network description here, include number of disks 
			on all nodes ,maybe add a diagram]}
			
			\subsubsection{Single-threaded evaluation}
			
			The simplest execution scenario is single-threaded, where memory 
			bandwidth and disk bandwidth are dedicated. Our goal is to 
			compare the detailed simulation results of each task in the pipeline 
			including read/write time, memory status and then validate the correctness 
			of the simulation model. 

			The pipeline used in this experiment consists of three tasks, 
			each task reads a file, increases every byte and writes output file 
			to local disk. 
			We use the input sizes of 20~GB, 50~GB, 75~GB and 100~GB, run 
			the pipeline on the cluster, measure the CPU time of the tasks  
			with each input and use it to simulate CPU time in the simulators .

			We simulate the pipeline with the Python prototype simulator, 
			a simulator with original WRENCH and another with WRENCH-Ext.  
			
			\subsubsection{Multi-threaded evaluation}

			In the second experiment, we evaluate the simulator in a concurrent I/O, 
			bandwidth-sharing scenario with a multi-threaded application. 			
			We vary the number of concurrent pipelines running on a multi-core host.  
			
			All the task in the pipelines read input files from and write output files 
			to a local disk. 
			As a compute node of the cluster has 32 cores CPU and 256~GB of RAM,  
			we create 32 input files with the size of 3~GB each and vary the number of 
			concurrent pipelines from 1 to 32. 
			
			Because implementation of a multi-threaded simulator in Python is 
			expensive, and we have our model implemented in WRENCH, 
			we only compare the results of simulators with original WRENCH, 
			WRENCH-Ext and real pipelines in this experiment. 
			The results are compared in total makespan of the pipelines, 
			cumulative read time and cumulative write time.
			
			\subsubsection{Evaluation on storage types}
			
		    Since different storage types should also be taken into account, 
		    we conduct another experiment with the same scenario as 
		    in the multi-threaded experiment, except that files are stored on a 
		    shared file system instead of local disk. 
		    
		    \textcolor{red}{[Description of Lustre on the cluster and how it's 
		    simulated]}. 
		    
		    \subsubsection{Simulation of a real application}

		    Finally, we consider a real pipeline from the neuroimaging domain. 			
			\textcolor{red}{[Description of the real pipeline with nighres 
			(including the workflow engine if it's Dask)]}.  			
			
			We simulate this pipeline with simulators using original WRENCH and 
			WRENCH-Ext. 
			The simulation results are then compared with the real results. 
			Because our work focuses on I/O time, we assume that CPU time is 
			correctly modeled and use the CPU time measured in the real pipelines 
			to setup our simulated workflow. The results are compared in terms of 
			read/write time and total makespan.

	\section{Results}
	
		\begin{itemize}

			\item Quantitative results: 
				\begin{itemize}
					\item Relative errors of simulation time and memory used compared to real results.
					\item Simulation time compared to SimGrid and original WRENCH.
				\end{itemize} 

			\item Ability of the model to generalize memory trends (dirty data, cache used) and disk throughput.

		\subsection{Single-threaded experiment}
			
		\subsection{Multi-threaded experiment}
		
		\subsection{Remote storage}
		
		\subsection{Real application}

		\end{itemize}

	\section{Discussion and Future Work}
		\begin{itemize}
			\item Read order: disk, inactive list, then active list. 
			\item Cache eviction mechanism not evicting active list, leads to evicting file being written.
			\item Sensitivity of the simulator on the variation of memory and disk bandwidth. 
			\item Our model does not support random I/O and readahead.
			\item Disk state save/restore when hosts are turned on/off.
		\end{itemize}
	\section{CFI for cluster}
\bibliographystyle{plain}
\bibliography{citation}

\end{document}
