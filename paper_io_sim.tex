\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}
\usepackage{hyperref}

\algblock{Input}{EndInput}
\algnotext{EndInput}
\algblock{Output}{EndOutput}
\algnotext{EndOutput}
\newcommand{\Desc}[2]{\State \makebox[2em][l]{#1}#2}

\newcommand{\tristan}[1]{\color{orange}\textbf{From Tristan:}#1\color{black}}
\newcommand{\valerie}[1]{\color{blue}\textbf{From Valerie:}#1\color{black}}


\begin{document}
\title{I/O simulation model with Linux page cache, integration and evaluation in SimGrid framework}

\author{\IEEEauthorblockN{Hoang-Dung Do, Val\'erie Hayot-Sasson, Tristan Glatard
  }\\
  \IEEEauthorblockA{
    Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada
  }
}

\maketitle

	\begin{abstract}
		\begin{itemize}
			\item The I/O bottleneck in HPC and the need of experiments.
			\item HPC experiment frameworks and advantages of SimGrid.
			\item The missing of the ability to simulate page cache, the goal of this paper.
			\item Principle of the simulator, experiment scenarios and comparisons.
			\item Brief discussion on results and future work.
		\end{itemize}
	\end{abstract}

	\section{Introduction}
		\begin{itemize}
			\item HPC, the bottleneck in I/O and the demand of HPC experiments. 
			\item Difficulties in conducting high performance computing experiments and the need of simulation frameworks.
			\item Existing experiment methods, simulators, simulation frameworks. The advantages of SimGrid compared to others \cite{casanova2008, lebre2015}. The missing of the ability to simulate page cache in SimGrid \cite{lebre2015}.
			\item The objective of the paper: Add capability to simulate I/O with page cache in SimGrid.
		\end{itemize}
	\section{Related Work}			
		
		\subsection{Page cache}
			\begin{itemize}
				\item What is page cache? How it works \cite{linuxdev3rd2010}. Effects and importance of page cache.
				\item Introduce some existing strategies with some highlighted pros and cons.
				\item Current implementation in Linux and some reasons why it is chosen to be implemented (implementation complexity, effectiveness, overhead, etc) \cite{linuxdev3rd2010}
				\item \tristan{LRU lists are mentioned in the next section, they should be described here.}
			\end{itemize}									

		\subsection{Simulators}
			\begin{itemize}
				\item Discuss some existing methods, simulation frameworks to conduct HPC experiments. Compare pros and cons (accuracy, simulation time, usability) of some simulators (SimGrid, GridSim).
				\item Related development: RAM energy consumption \cite{gill2019} \cite{ouarnoughi2017} 
				\item Discuss the pros of SimGrid and the reasons why we chose it to extend. (Section 2.2.2 in \cite{casanova2014})
				\item \tristan{Mention the status of storage simulation in simulators, so that we understand from Fig 1 that storage devices are available}
			\end{itemize}
			
	\section{Method}

    \valerie{ In this section, you use terms like, ``file read and write'', ``memory I/O''
    and ``Kernel I/O''. I think it's important that you define them in your introduction
and make sure that these terms are actually distinct. By ``file read and write'', i'm guessing you essentially
mean disk+memory I/O, by memory I/O, you must mean I/O to page cache (and not anonymous memory management). I'm not sure
what you mean by ``kernel I/O'' unless it's the whole ensemble of i/o componenets?}
		In this section, we present our approach to model file read and write,
		memory I/O, cache eviction and data flushing mechanisms implemented in
		the Linux kernel. We also detail the design of our simulators and their
		implementation. Finally, we describe experimental scenarios to evaluate
		our simulation model on real applications using a standalone Python
		prototype, the current SimGrid simulator, and our extension of SimGrid.

		\subsection{Principle of the simulator}
	
            We separate our simulation model in two components, the IO
			Controller and the Memory Manager, which together simulate Linux
			kernel I/Os (Figure~\ref{fig:interaction}). The Memory Manager
			simulates memory I/O, flushing, and cache eviction, whereas the IO
			Controller simulates file reads and writes. Simulated applications
			can simply interact with the IO Manager instead of calling simulated
			storage directly as is the case with current simulators.

			\begin{figure}
   				\centering
   				\includegraphics[width=0.85\columnwidth]{figures/interaction.pdf}
   				\caption{Overview of the simulated write-back page cache}\label{fig:interaction}
			\end{figure}	

			\subsubsection{Memory simulation model}
			
			We leverage existing storage simulation models to simulate the memory,
			which is characterized by storage capacity and read and write bandwidths. 
			This can be done when disks are modeled with distinct read/write 
            bandwidths and negligible disk latency \valerie{Does this mean we assume that all file i/o within a pipeline occurs on relatively large files?}, which is applicable to memory 
			\cite{lebre2015}. In addition, leveraging existing disk models allows us 
			to take advantage of bandwidth sharing between concurrent disk I/O to 
			simulate parallel memory accesses.\tristan{we should refer to related work, 
            see comment there} \valerie{Agreed. It's unclear how existing storage models enable this at the moment}.

			We introduce the concept of a data block as a unit to represent data
			cached in memory. A data block is a subset of file pages stored in
            page cache that were accessed in the same read or write operation. 
			A data block has information about file name, block size, last access 
			time, and a dirty flag that represents whether the data is clean (0) 
			or dirty (1). 
			A given file can have multiple data blocks in page cache. In
			addition, a data block can be split into an arbitrary number of
			smaller blocks, and data blocks can be merged together.
			
			We simulate the kernel LRU lists of file pages
			with two lists of data blocks: an active list and 
			an inactive list, ordered by last access time (earliest first, see 
			Figure~\ref{fig:lrulist}).
			As in the kernel, our simulator keeps the size of the active list under
			twice the size of the inactive list by moving least recently 
            used data blocks from the active list to the inactive list.\valerie{should maybe explain this in the datablock section}
			Modeling page cache LRU lists as lists of data blocks reduces the
			overhead of the simulator, as data blocks are obviously less
			numerous than file pages, while preserving the accuracy in I/O time
			simulation \tristan{how do you know that, can you provide an argument?}.

			\begin{figure}
   				\centering
   				\includegraphics[width=\columnwidth]{figures/lru_lists.pdf}
   				\caption{Simulation of page cache LRU lists with data blocks}\label{fig:lrulist}
			\end{figure}	
			

            \valerie{I think you might need to reorder the paragraphs. First one would be
            to explain what and dirty/clean blocks, 2. what happens when data is read, 3. what happens
            when data is written, and maybe mention file accesses that are not read/write}
            The simulated LRU lists are used as follows. When a file is read, 
            file data \valerie{are you talking about the file you just read or file data already in cache?} can be partially cached, completely cached, or uncached. 
            \valerie{rewrite: Cached data can be composed of data blocks existing in either or both
            of the page cache LRU lists. Cached blocks reside in the inactive list
            unless they have been recently been reaccessed. Cached blocks located on the inactive
            list that are subsequently accessed, will be moved to the top of the active list.}
            For cached data, there are existing data blocks representing
			this data in two page cache LRU lists. 
			As this cached data is accessed again, these blocks are moved to 
			the active list, dirty blocks are merged into a bigger dirty block, 
			while clean data blocks are merged into another clean block. 
			If data read is not in cache, a clean block representing uncached data 
			is created and put into the top of the inactive list. 
			
			In the case of writing data, we can assume that all written data 
			is uncached. After data is written, some can remain dirty, some is 
			clean after being flushed. 
			Thus, at most two data blocks can be added, one is clean and one 
			is dirty, to the inactive list.
			
            As done in the kernel, we ensure the size of the active list does not exceed twice the size of 
			the inactive list 
			\cite{gorman2004understanding, linuxdev3rd2010}. 
			This is done by moving least recently used blocks in the active list 
			to the inactive list when the active list grows. If only a part of a data 
			block needs to be moved, the block is split.

			Next, we simulate data flushing mechanism, which writes back 
			dirty data to disk. 
			The data flushing simulation function traverses the sorted inactive first, 
			and then the active list, looks for least recently used dirty blocks, 
			writes them to disk through the storage device model, and 
			sets dirty flags of these blocks to 0 until the total flushed amount 
			reaches the amount to be flushed or there is no dirty data left in cache. 
			If the last flushed block is not entirely flushed, it is split into 
			two blocks, one that is flushed and one that remains dirty.
			The flushing time is simulated with storage model.
			
			For periodical flushing, our simulator searches in two page cache 
			LRU lists and flushes all the expired dirty data blocks. 
			A dirty data block is considered expired if the time from its last access 
			to current simulated time is longer than the predefined expire time. 
			Periodical flushing can be concurrent with cache read/write, 
			and sequential with disk read/write. When cache accesses and 
			periodical flushing are concurrent, flushing time is not added to simulated 
			time but the amount of flushed data is limited by flushing time. 
			When data is flushed in parallel with disk accesses, the concurrency 
			is handled by the storage model used as memory, and flushing time 
			is simulated with the amount of flushed data by this model. 
				
			Similarly to data flushing, our cache eviction simulation function frees up 
			the page cache by traversing and deleting least recently used clean 
			data blocks in the inactive list.
			The amount of evicted data is passed in and data blocks are deleted 
			from the inactive list until total evicted data reaches the amount 
			passed in, or there is no clean block left in the list.
			If the last evicted block is not entirely evicted, the block is split, 
			and only one block is deleted.
			Our cache eviction simulation does not add up to the simulated time 
			since cache eviction time is negligible in real systems.			

			\subsubsection{File I/O simulation model}			
			
			Simulated applications can simply request for a file read 
			or a file write by calling the IO Controller instead of 
			the storage simulation models. 
			\tristan{this should move, not sure where for now: Because every computer has its own memory and system configurations, 
			one IO Controller object and one Memory Manager object are created 
			for each simulated machine.}
			
			To read or write a file, a simulated application sends a request to the 
			IO Controller instance on the node where the requested file is stored.
			Based on the memory status in Memory Manager, the IOController 
			orchestrates cache reads and writes, disk reads and writes, flushing, cache eviction 
			 and returns to the simulated application.
			
			\begin{algorithm}\caption{File read simulation}\label{alg:read}
				\small
				\begin{algorithmic}[1]
					\Input
        				\Desc{fn}{file name}
        				\Desc{fs}{file size (assumed to fit in memory)}
						\Desc{mm}{MemoryManager object}
						\Desc{sm}{storage simulation model}
   					\EndInput
					\State mm.flush(2*fs - mm.cached(fn) - mm.evictable\_mem) 
					\State mm.evict(2*fs - mm.cached(fn) - mm.free\_mem) 
					\If {mm.cached(fn) $>$ 0}  \Comment{Read file cached data}
					    \State mm.periodic\_flush() \Comment{Flush during cache read}
    					\State mm.cache\_read(fn) 
					\EndIf
					\If {mm.cached(fn) $<$ fs} \Comment{Read uncached data from disk}
						\State mm.periodic\_flush() \Comment{Sequential with disk read}
						\State mm.add\_to\_cache(fs - mm.cached(fn), fn)
    					\State sm.read(fs - mm.cached(fn))
					\EndIf					
					
				\end{algorithmic}
			\end{algorithm}			
			
			The algorithm to simulate file reads is described in 
			Algorithm~\ref{alg:read}.   
			In Linux systems, data can be either read from cache with memory bandwidth 
			or read from disk with disk bandwidth. 
			Therefore, total read time can be considered as sequential reads with 
            these different bandwidths\valerie{i understand what you're saying,
            but maybe it'd be better to say: ``Therefore read time can be considered as the sum of the time required to write to all devices''.
            Can you clarify if disk+memory writes are not concurrent }, enabling us to generalize reading process as 
			two separate phases, cache read and disk read. 
			When a simulated application reads a file, it sends a file read request 
			to IO Controller. 
			First, if there is not enough memory available to store two
			copies of the file (one in cache, one in anonymous memory), the
			Memory Manager is called to flush dirty data to disk and simulate 
			flushing time with memory model (line 6).
			This flushing is complemented by eviction by the Memory Manager (line 7). 
			Next, if the file is cached, either partially or entirely, data is read from 
			cache first. Periodical flushing is triggered at the beginning of the 
			cache read to flush dirty data if some dirty data is expired (line 9).
			At the same time, Memory Manager is called to read data from cache, 
			and update the corresponding data blocks in cache (line 10). 
			The concurrency of periodical flushing and cache read is handled and 
			time is simulated by the memory and disk models.
			
			If file is not cached or partially cached, a disk read is required (line 12). 
			Periodical flushing is called again at the beginning of the disk read 
			(line 13), the storage model simulates the disk write with the remaining 
            amount of data (line 14) \valerie{Has this line been removed?}, and data read from disk is added to cache 
			(line 15). Similar to cache read period, disk bandwidth sharing is handled 
			and time is simulated by storage models.

			\begin{algorithm}\caption{File write simulation}\label{alg:write}
				\small
				\begin{algorithmic}[1]
					\Input
        				\Desc{fn}{file name}
        				\Desc{fs}{file size}
						\Desc{mm}{MemoryManager object}
						\Desc{sm}{storage simulation model}
   					\EndInput
   					\State mm.periodic\_flush()
					\State remain\_dirty = dirty\_ratio * mm.avail\_mem - mm.dirty
					\State mem\_amt = min(fs, remain\_dirty)
					\If {remain\_dirty $>$ 0} \Comment{Write with memory bandwidth}
    					\State mm.evict(mem\_amt - mm.free\_mem)
    					\State mm.write(fn, mem\_amt) 
    				\EndIf
					\If {mem\_amt $<$ fs}  \Comment{Write with disk bandwidth}
						\State mm.flush(fs - mem\_amt)  \Comment{Concurrent with disk write}
						\State mm.evict(fs - mem\_amt  - mm.free\_mem) 
						\State mm.add\_to\_cache(fn, min(fs - mem\_amt, mm.free\_mem))
						\State sm.write(fs - mem\_amt)
					\EndIf
					
				\end{algorithmic}
			\end{algorithm}

            \valerie{I think we need to take periodic flush out as it shouldn't be added to the write time}


			Algorithm~\ref{alg:write} describes our simulation of file write. 
			Initially, periodical flushing is called to asynchronously flush expired 
			dirty data at the beginning of the write (line 6).
			Similar to the file read simulation, a file can also be written with 
            two different bandwidths, memory bandwidth and disk bandwidth \valerie{can't we just say cached to memory and written to disk and assume the bandwidths are implied?}. 
			Data can be written with memory bandwidth before the amount 
			of dirty data reaches the dirty\_ratio. Thus, our algorithm initially checks 
			the amount of data that can be written with memory write 
			bandwidth (line 7-8).
			If this amount is greater than 0, MemoryManager is asked to evict 
			data from cache to accommodate the written data if needed (line 10). 
			Then, cache write is simulated with memory model and data written is 
			added to page cache LRU lists (11).
			The amount of data that is not written with memory bandwidth is written 
			with disk bandwidth (line 13). 
			Now, the dirty ratio threshold is reached, we have to flush and evict 
			data from cache as much as possible so that 
			the remaining data can be written to cache (line 14-15). 
			Here, MemoryManager does not guarantee that the amount of flushed 
			and evicted data is enough for the remaining data. In the case it is 
			less than the required amount, the remaining data is written to cache 
			but then can be flushed and evicted right away. Thus, the amount 
			of remaining data that is written and kept in cache after the write 
			is limited by the amount free memory (line 16). 
			Finally, the disk write is simulated with the remaining amount of 
			data (line 17). As in the simulation of file read, all the concurrent 
			cache write and disk write are simulated by memory and disk models.
			
		\subsection{Implementation}

			To validate our simulation model, we create a simple prototype
			simulator independent of existing simulation frameworks and libraries. 
			This enables us to evaluate the accuracy and correctness of our 
			model in a simple scenario before integrating it in the more complex SimGrid environment. 
			In this prototype we use the following basic storage model, used for both memory and disk: 
			\begin{align*}
				& t_{r} = D / b_r \\ 
				& t_{w} = D / b_w\
			\end{align*}		
			
			where:
			\begin{itemize}
				\item $t_{r}$ is the data read time
				\item $t_{w}$ is the data write time
				\item $D$ is the amount of data to read/write
				\item $b_r$ is the read bandwidth of the device
				\item $b_w$ is the write bandwidth of the device
			\end{itemize}			

			Since bandwidth sharing is not simulated, this prototype doesn't support 
			concurrency: it is limited to single-threaded pipelines running on systems 
			with a single-core CPU. We use it in a first validation of our simulation 
			model, against a real sequential pipeline running on a real system.
			Source code of our Python prototype is available at 
            \url{https://github.com/big-data-lab-team/paper-io-simulation/tree/master/exp/pysim}
			
			Having our model validated, we create simulators for different use cases 
			using the current version of SimGrid and the SimGrid version that is 
            extended with our model \valerie{I think you should give the extended simgrid a new name ``SimGridExt'' or something just so you can use that name in you figures}. To simulate our pipelines and system, we leverage 
			the Workflow Management System Simulation Workbench 
			(WRENCH~\cite{wrench}), a framework  built over SimGrid.
		    We compare the results of the simulators with
			original SimGrid and extended SimGrid with the results of real
			pipelines on a real system. 
			
			\textcolor{red}{[Description, implementation of our SimGrid extension]}. 
		
			We use Python 3.7 to implement the simple simulator, SimGrid 3.25
			and Wrench 1.6 for SimGrid simulators. SimGrid source code is
			available at \url{https://framagit.org/simgrid/simgrid}, and WRENCH
			is at \url{https://github.com/wrench-project/wrench}.
			
		\subsection{Experiments}
		
        The goal of our experiments is to evaluate our write-back cache \valerie{This is the first time the write-back cache is mentioned apart from in a figure caption. definitely should be described in your intro alongside write-through and direct I/O}
			simulation in single-threaded and multi-threaded applications
			accessing data on different types of file system. We use a pipeline
			of sequential tasks where each task reads an input file, increments
			every byte of the input to emulate real processing and avoid caching
			effects, and writes the result to disk. The output of the previous
			task is the input of the next task. We also use a real application 
            pipeline \valerie{Maybe briefly describe this application's purpose (e.g. a standard fMRI preprocessing piepeline) or something of the sort}, to evaluate the applicability of our simulation model. 
			
			We use our dedicated cluster hosted at Concordia University to conduct 
			the experiments. The cluster consists of one login node, 8 compute nodes 
			and 4 storage nodes connected with two network switches. The login node 
			has one Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz, 128~GiB of RAM, 1.8TB 
			of storage of XFS file system, 13~GB of tmpfs file system and 385 TB of 
			Lustre shared file system. Each compute node has two 16 cores Intel(R) 
			Xeon(R) Gold 6130 CPU @ 2.10GHz, 256~GB of RAM and 6 SSDs, 450~GB each 
			with XFS file system, 378~GB of tmpfs and 126~GB of devtmpfs file system.
			Lustre file system is configured with one metadata target with 854~GB 
			of storage, 44 object storage targets with 8.7~TB of storage each. 
			The cluster is run on CentOS 8.1 with the Slurm Workload Manager installed. 
			We use \textbf{\textit{atop}} and \textbf{\textit{collectl}} as tools to 
			monitor and collect data of memory, page cache status and disk throughput 
			on the cluster. 
			\textcolor{red}{[Add network description here, include number of disks 
			on all nodes ,maybe add a diagram]}
			
			\subsubsection{Single-threaded evaluation}
			
			The simplest execution scenario is single-threaded, where memory 
			bandwidth and disk bandwidth are dedicated. Our goal is to 
			compare the detailed simulation results of each task in the pipeline 
			including read/write time, memory status and then validate the correctness 
			of the simulation model. 

			The pipeline used in this experiment consists of three tasks, 
			each task reads a file, increases every byte and writes output file 
			to local disk. 
            \valerie{Be careful because you've been switching between GiB and GB. Make sure your input sizes are not GiB}
			We use the input sizes of 20~GB, 50~GB, 75~GB and 100~GB, run 
			the pipeline on the cluster, measure the CPU time of the tasks  
			with each input and use it to simulate CPU time in the simulators .

			We simulate the pipeline with the Python prototype simulator, 
			a simulator implemented with original SimGrid and a simulator implemented 
			with SimGrid extended with our model.  
			
			\subsubsection{Multi-threaded evaluation}

			In the second experiment, we evaluate the simulator in a concurrent I/O, 
			bandwidth-sharing scenario with a multi-threaded application. 			
			We vary the number of concurrent pipelines running on a multi-core host.  
			
			All the task in the pipelines read input files from and write output files 
			to a local disk. 
			As a compute node of the cluster has 32 cores CPU and 256~GB of RAM,  
			we create 32 input files with the size of 3~GB each and vary the number of 
			concurrent pipelines from 1 to 32. 
			
			Because implementation of a multi-threaded simulator  in Python is 
			expensive, and we have our model integrated in SimGrid, 
			we only compare the results of simulators with original SimGrid, 
			SimGrid integrated with our model and and real pipelines in this 
			experiment. The results are compared in total makespan of the pipelines, 
			cumulative read time and cumulative write time.
			
			\subsubsection{Evaluation on storage types}
			
		    Since different storage types should also be taken into account, 
		    we conduct another experiment with the same scenario as 
		    in the multi-threaded experiment, except that files are stored on a 
		    shared file system instead of local disk. 
		    
		    \textcolor{red}{[Description of Lustre on the cluster and how it's 
		    simulated]}. 
		    
		    \subsubsection{Simulation of a real application}

		    Finally, we consider a real pipeline from the neuroimaging domain. 			
			\textcolor{red}{[Description of the real pipeline with nighres 
			(including the workflow engine if it's Dask)]}.  			
			
			We simulate this pipeline with a simulator with original SimGrid and 
			extended SimGrid. 
			The simulation results are then compared with the real results. 
			Because our work focuses on I/O time, we assume that CPU time is 
			correctly modeled and use the CPU time measured in the real pipelines 
			to setup our simulated workflow. The results are compared in terms of 
			I/O time and total makespan.

	\section{Results}
	
		\begin{itemize}

			\item Quantitative results: 
				\begin{itemize}
					\item Errors of simulation time and memory used compared to real results.
					\item Simulation time compared to baseline SimGrid.
				\end{itemize} 

			\item Ability of the model to generalize memory trends (dirty data, cache used) and disk throughput.

		\end{itemize}

	\section{Discussion and Future Work}
		\begin{itemize}
			\item Sensitivity of the simulator on the variation of memory and disk bandwidth. 
		\end{itemize}
	\section{CFI for cluster}
\bibliographystyle{plain}
\bibliography{citation}

\end{document}
