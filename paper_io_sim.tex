\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{ulem}
\usepackage{tabularx}

% Fix link colors
\hypersetup{
    colorlinks = true,
    linkcolor=red,
    citecolor=red,
    urlcolor=blue,
    linktocpage % so that page numbers are clickable in toc
}

\algblock{Input}{EndInput}
\algnotext{EndInput}
\algblock{Output}{EndOutput}
\algnotext{EndOutput}
\newcommand{\Desc}[2]{\State \makebox[2em][l]{#1}#2}

\newcommand{\tristan}[1]{\color{orange}\textbf{From Tristan:}#1\color{black}}
\newcommand{\english}[1]{\uwave{#1}}

\newcommand{\valerie}[1]{\color{blue}\textbf{From Valerie:}#1\color{black}}


\begin{document}

\title{Simulating the Linux page cache in distributed applications with the WRENCH library}

\author{\IEEEauthorblockN{Hoang-Dung Do$^1$, Val\'erie Hayot-Sasson$^1$, Rafael Ferreira da Silva$^3$, Henri Casanova$^2$, Tristan Glatard$^1$
  }\\
  \IEEEauthorblockA{
    $^1$Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada\\
     $^2$Department of Information and Computer Sciences, University of Hawaiâ€˜i at Manoa, USA\\
     $^3$Department of Computer Science, University of Southern California, Los Angeles, USA
  }
}

\maketitle

	\begin{abstract}

		
		The rapid emergence of Big Data seen in recent years has resulted in a 
		growing need for efficient data storing and optimized data analysis techniques. 
		Despite relying on large computing infrastructures with powerful computing capability 
		to process these large datasets, an I/O bottleneck remains. 
        One widely-used approach to minimize the costs of data transfers during
        regular use is through the employment of the page cache.
		While many simulation tools have been developed to 
		support studies in computing systems optimization, page cache simulation 
		has not been fully captured or remains absent from these simulators. 
		The inability to simulate I/O with page cache leads to inaccurate 
		simulations in many situations, especially when dealing with data intensive applications. 
		
		In this paper, we propose a I/O simulation model that characterizes the key features of the page cache.
		We have implemented the model with SimGrid, a simulation framework for 
		distributed systems, using WRENCH, a SimGrid-based workflow simulator. 
        Our simulator is compatible with both single-threaded and multithreaded applications,
        and can simulate both writeback and writethrough cache that can factor into local
        and network-based filesystems.
		The results show that our simulator not only achieves higher accuracy than 
		original WRENCH, but also effectively simulates memory contents and 
		page cache during I/O operations. 
		Through this study, our effort is to contribute an analytical model 
		of the page cache, which can also be integrated to other simulators 
		to improve I/O simulation accuracy. 
		
	\end{abstract}		 

	\section{Introduction}
		
        The Linux Page Cache, introduced in kernel version 2.4, plays an important role in
        the reduction of filesystem data transfer costs~\cite{pagecache}. With the page cache, previously
        read data can be reread directly from memory and written data can be written to
        memory before being asynchronously flushed to disk, resulting in improved I/O performance
        on slower storage devices. The performance benefits that can be obtained from using the page
        cache are determined by many factors including the total amount of memory,
        the amount of data being written (i.e. dirty data), and the percentage of available memory available for
        written data. All these factors are important when determining the impact of data transfers on
        application performance, particularly in data intensive applications.

        Characterizing the performance of applications directly on computing infrastructure
        has various shortcomings, which include, limited and shared resources, operational
        costs, unstable environments, and the difficulties in producing reproducible results.
        Simulations address these concerns by providing abstractions to the performance of computer
        hardware, such as CPU, network and storage. As a result, simulations provide and
        cost-effective, fast, easy and reproducible  way evaluate application performance
        on different infrastructures without having to execute the code.


        Page cache simulation is crucial when simulating data intensive applications,
        which would rely on cache hits and writing to memory for additional performance.
        While existing simulators can adequately characterize many different computing environments,
        they lack much of the ability to adequately describe the page cache. Some
        simulators try to simulator I/O with page cache, but fail to capture the key features such
        as dirty data and cache eviction policies~\cite{nunez2012simcan,nunez2012icancloud}. 
        Other simulators, such as~\cite{xu2018}, capture the important feature of the
        page cache, but are domain-specific. 

		%However, there is a trade-off between accuracy and scalability, which has been 
		%indicated in some reviews \cite{casanova2014simgrid} \cite{byrne2017review}. 
		%This means that in most cicumstances, users have to, more or less, sacrifice 
		%accuracy for the scalability and performance when their platforms grow to 
		%hundreds or thousands nodes. 

        In this paper, we present WRENCH-Cache, a simulation model of I/O with page
        cache incorporated into WRENCH, a workflow management system simulator based of
        the popular SimGrid framework. The goal of WRENCH-Cache is to provide users
        with an accurate representation of a workflow execution, when file system page
        cache is used. WRENCH-Cache is able to simulate both writeback and writethrough
        cache which would normally be available to local and NFS-based filesystems.
		
    \section{Related Work}        
    \label{relatedwork}    
        
        \subsection{Page cache}                            
            
            Linux implements a disk cache called \textit{page cache}. 
            It contains pages referring to physical pages on disk \cite{linuxdev3rd2010}.
            The goal of the page cache is to avoid disk I/O by keeping data in memory 
            for later accesses, minimize I/O time since disk accesses are considerably 
            slower than memory accesses.
            When the kernel starts a read operation, it checks if required data is in page cache.
            If yes, data is read from memory. This is called a \textit{cache hit}. 
            Otherwise, data is read from disk and placed in page cache.
            For writing, page cache is implemented with three different strategies. 
            The first is \textit{direct~I/O} or \textit{no-write}, which means the page cache 
            is not involved in write operations. 
            The second is \textit{write-through}, in which data goes \textit{through} 
            page cache to disk. In this case, data is written to both page cache and disk. 
            In the third strategy, called \textit{writeback}, data are written directly 
            to disk as \textit{dirty} pages and these pages will be periodically flushed 
            to disk later by a \textit{writeback} process and then marked as not dirty. 
            The writeback strategy is considered to outperform write-through as well as 
            direct I/O as it delays disk writes and perform a bulk write later.
            \cite{linuxdev3rd2010}. 
            
            In writeback, one of the keys is the cache eviction mechanism, which is
            triggered when available memory is insufficient to removes pages 
            from page cache. 
            Because dirty pages are data written to page cache but yet to be 
            written to disk, dirty pages must be kept in page cache and only clean pages 
            are available for eviction.
            If the clean pages are inadequate, dirty pages are flushed to disk, 
            marked as clean and available for eviction. 
            There is another type of flushing mechanism, which is 
            \textit{periodical flushing}, called periodically in predefined intervals. 
            Periodical flushing only flushes expired dirty pages, which remain dirty in 
            page cache longer than an expiration time configured in the kernel. 
            Different cache eviction algorithms have also been developed and proposed
            \cite{owda2014comparison}.
            In Linux kernel, a \textit{two lists strategy (LRU/2)} is implemented based on 
            the least-recently-used (LRU) scheme, with an \textit{active list} and 
            an \textit{inactive list}. 
            If accessed pages are not in the page cache, they are added to the inactive list. 
            If pages are accessed when residing in the inactive list, they will be moved from 
            inactive list to active list. 
            The lists are also kept balanced by moving pages from the active list
            to the inactive list when the active list grows too large.
            Thus, the active list only contains pages which are accessed more than once 
            and not evictable, while the inactive list includes pages accessed once only, 
            or pages accessed more than once but moved from the active list. 
            The lists work in LRU manner, pages are added to the tail and removed 
            from the top. 
             
        \subsection{Simulators}
            
            Many simulation frameworks and applications have been developed 
            in recent years to support studies in grids, cloud, HPC, peer-to-peer 
            networks, volunteer computers.                     
            These simulators are built with models of resources such as CPU, network, 
            storage with different approaches and levels of details in different simulators. 
            The detail level of the model causes a conflict between accuracy and scalability, 
            and these simulators usually trade of one for the other 
            \cite{casanova2014simgrid}. 
            For example, some simulators are built with packet-level network models 
            such as \textit{CloudSim} and \textit{iCanCloud}, or storage model that 
            simulates the operation of storage hardware in \textit{DiskSim} 
            \cite{casanova2014simgrid}. 
            Fine-grained models allow simulators to achieve high accuracy, 
            but at the same time lead to scalability issues due to long simulation time 
            when simulated platforms and applications grow. 
            SimGrid is an open source simulation platform that has been developed 
            not only to balance the trade-off between accuracy and scalability, 
            but also to achieve versatility, which allows the use of the framework 
            to simulate applications in multiple domains \cite{casanova2014simgrid}.
            
            Despite the fact that many sophisticated models for resources have been 
            integrated in simulators, and the page cache shows apparent impacts on I/O
            performance, the simulation model of page cache is still missing in most 
            of simulators.
            For example, SimGrid only simulates I/O operations with storage bandwidths 
            and capacity. 
            SIMCAN is a simulation framework that models page cache by storing data 
            accessed on disk in a block cache \cite{nunez2012simcan}. 
            Page cache is also modeled in iCanCloud simulator with a component in charge 
            of managing memory accesses and cached data \cite{nunez2012icancloud}, 
            but one of the issues is scalability due to its sophisticated model.
            Ultimately, non of these simulators provide any cache write strategy, 
            cache eviction mechanism with dirty/clean data and the page cache LRU lists. 
            Cache replacement policies is applied in \cite{xu2018saving} to simulate 
            in-memory caching, but for the study in energy consumption of multi-tier 
            heterogenous network.
            
            In this study, we chose SimGrid to implement our model of I/O with page cache 
            due to the balancing between accuracy and scalibility, the versatility allowing it 
            to be adopted in different domains. 
            In addition, it is an actively maintained open-scource framework with 
            modular design, which enables the framework to be extended with new models. 
            The main issue users have to face when using SimGrid is everything has to be 
            implemented from scratch due to its low-level API. 
            To ease this challenge, WRENCH, another open-source library, is built on top 
            of SimGrid to abstract the low-level logic and expose a higher abstraction 
            level API \cite{casanova2018wrench}. 
            We integrate our model in WRENCH and use the SimGrid API to interact with 
            simulated storage resource.
            This approach allows us to leverage existing features in WRENCH and SimGrid.
            Firstly, we can take advantage of the process control, synchronization of 
            resource consumption by concurrent processes provided by the simulation core 
            in SimGrid. 
            Secondly, we can reuse the existing SimGrid storage model with capacity 
            and bandwidth properties, which are similar to the characteristics of memory.
            And finally, WRENCH provides a high level abstraction, which help us drop 
            the burden of implementation. 
            
    \section{Method}
    \label{method}
    In this section, we present our page cache simulation model for file I/O,
    flusing, and eviction. We detail the design and implementation of two simulators:
    (1) a standalone Python prototype to evaluate the simulation model independently of any other 
    tool, and (2) WRENCH-cache, an extension of WRENCH with our page cache model.
    We also describe experimental scenarios to evaluate our simulators in various configurations.
     
    We separate our simulation model in two components, the IO
    Controller and the Memory Manager, which together simulate 
    file reads and writes in Linux (Figure~\ref{fig:interaction}). 
    To read or write a file chunk, a simulated application sends a
    request to the IOController. The IOController interacts as needed with
    the Memory Manager to free memory through flushing or eviction,
    and to read or write to cache. The Memory Manager
    implements these operations, simulates periodical flushing
    and eviction from cache, and reads or writes to disk when necessary.
    \begin{figure}
           \centering
           \includegraphics[width=0.85\columnwidth]{figures/interaction.pdf}
           \caption{Overview of the simulated writeback page cache.
           Applications send file read or write requests to the
           IOController that orchestrates flushing, eviction, cache
           and disk accesses with the MemoryManager. Storage
           devices (memory and disk) simulate file transfers with
           bandwidth sharing.}
           \label{fig:interaction}
    \end{figure}

    \subsection{Memory Manager}

    The Memory Manager simulates two parallel threads: the main one
    implements flushing, eviction, and cache I/Os synchronously, whereas
    the second one, operating in the background, periodically searches for
    expired dirty data in page cache LRU lists and flushes them to disk. We
    use existing storage simulation models~\cite{lebre2015} to simulate
    memory, characterized by its storage capacity, read and write
    bandwidths, and latency. Reusing existing storage models allows us to
    simulate bandwidth sharing between concurrent memory accesses.

    \subsubsection{Page cache LRU lists}

    In the Linux kernel, page cache LRU lists contain file pages. However, 
    due to the large number of file pages, managing lists of pages 
    induces substantial overheads.
    Therefore, we introduce the concept of a data block as a unit to represent data 
    cached in memory. A data block is a subset of file pages stored in
    page cache that were accessed in the same I/O operation. 
    A data block has information about file name, block size, last access 
    time, a dirty flag that represents whether the data is clean (0) 
    or dirty (1), and an entry (creation) time.
    Blocks can have different sizes and a given file can have multiple 
    data blocks in page cache. In addition, a data block can be split into an 
    arbitrary number of smaller blocks.
    \begin{figure}
           \centering
           \includegraphics[width=\columnwidth]{figures/lru_lists.pdf}
           \caption{Model of page cache LRU lists with data blocks.}    \label{fig:lrulist}
    \end{figure}    
    
    We model page cache LRU lists as 
    two lists of data blocks, an active list and an inactive list, both ordered by 
    last access time (earliest first, Figure~\ref{fig:lrulist}).
    As in the kernel, our simulator limits the size of the active list to
    twice the size of the inactive list, by moving least recently 
    used data blocks from the active list to the inactive list~\cite{gorman2004understanding, linuxdev3rd2010}.
    % Modeling page cache LRU lists as lists of data blocks reduces the
    % overhead of the simulator, as data blocks are obviously less
    % numerous than file pages. At the same time, it achieves the accuracy in 
    % I/O time simulation that can be seen in the later section.
    % From Tristan: I don't think the sentences above are necessary,
    % they are quite repetitive.

    At any given time, a file can be partially cached, completely cached,
    or uncached at all. Cached data blocks may reside in one or both of the
    page cache LRU lists. The first time they are accessed, blocks are
    added to the inactive list. On subsequent accesses, blocks of the
    inactive list are moved to \tristan{if they are moved then they can't
    be in both lists} the top of the active list. Cached blocks written to
    cache are marked as dirty until they are flushed to disk.

    \subsubsection{Reads and Writes}

    Our simulation model supports chunk-by-chunk file accesses
    with a user-defined chunk size. However, for simplicity, we assume that file pages are 
    accessed in a round-robin fashion rather than fully randomly. 
    Therefore, when a file is read, cached data is read only after all uncached data was read, and data from the inactive list is read
    before data from the active list
    (left to right in Figure~\ref{fig:read_order}).
    When a chunk of \emph{uncached} data is read, a new clean block is created 
    and put at the top of the inactive list. 
    When a chunk of \emph{cached} data is read, one or more existing data blocks in the LRU lists are accessed.
    If these blocks are clean, we merge them together, update the access time and size of the resulting block, 
    and put it in the active list. 
    If the blocks are dirty, we move them to the active list to preserve their entry time. 
    Because the chunk size and block size can be different, there are situations
    where a block is not entirely read. 
    In this case, the block is split in two smaller blocks and one of them is re-accessed.
    % From Tristan: not sure where to put this nor if it's necessary:
    % , in which chunks are read/written
    % until file is entirely read/written.
    \begin{figure}
           \centering
           \includegraphics[width=\columnwidth]{figures/read_order.pdf}
           \caption{File data read order. Data is read from left to right: uncached data 
           is read first, followed by data from the inactive list, and finally data from the active list.}
           \label{fig:read_order}
    \end{figure}    

    For file writes, we assume that all data to be written is 
    uncached. Thus, each time a chunk is written, we create a block of dirty data 
    and add it to the inactive list.

    \subsubsection{Flushing and eviction}

    The main simulated thread in the Memory Manager can flush the
    cache to write dirty data to disk. The data flushing simulation
    function takes the amount of data to flush as parameter. While
    the amount of data to flush is not reached and there are dirty
    blocks remaining in cache, this function traverses the sorted
    inactive list, then the sorted active list, and writes the
    least recently used dirty block to disk having set its dirty
    flag to 0. In case the amount of data to flush requires that a
    block be partially flushed, the block is split in two blocks,
    one that is flushed and one that remains dirty. Flushing time
    to disk is simulated with the storage model.

    Similarly to data flushing, our cache eviction simulation runs in
    the main thread. It frees up the page cache by traversing and deleting 
    least recently used clean data blocks in the inactive list.
    The amount of data to evict is passed as a parameter and data blocks are deleted 
    from the inactive list until total evicted data reaches the required amount,
    or until there is no clean block left in the list.
    If the last evicted block does not have to be entirely evicted, the block is split in two blocks,
    and only one of them is evicted.
    The cache eviction simulation does not contribute to the simulated time 
    since cache eviction time is negligible in real systems \tristan{a reference would be welcome}.
    
    \begin{algorithm}\caption{Periodical flushing simulation}\label{alg:pdflush}
        \small
        \begin{algorithmic}[1]
            \Input
                \Desc{in}{page cache inactive list}
                \Desc{ac}{page cache active list}
                \Desc{t}{predefined flushing time interval}
                \Desc{exp}{predefined expiration time}
                \Desc{sm}{storage simulation model}
               \EndInput
               \While{host is on}
                \State blocks = expired\_blocks(exp, in) + expired\_blocks(exp, ac) 
                \State flushing\_time = sm.write(blocks)
                \State flag blocks as clean data \tristan{a bit informal}
                \If{flushing\_time $<$ t}
                    \State sleep(t - flushing\_time)
                \EndIf \tristan{else?}
            \EndWhile
        \end{algorithmic}
    \end{algorithm}

    Periodical flushing is simulated in the Memory Manager
    background thread. As in the Linux kernel, a dirty block
    in our model is considered expired if (1) it is dirty, and (2)
    the duration since its entry time is longer than a
    predefined expiration time. 
    Periodical flushing is simulated as an infinite loop in which 
    the Memory Manager searches and flushes these expired dirty blocks to disk (Algorithm~\ref{alg:pdflush}). 
    % From Tristan: the algorithm is quite straightforward, I don't think this is 
    % necessary
    % In each repetition, Memory Manager finds expired dirty blocks in two 
    % page cache LRU lists (line 8), simulates writes of data of these blocks 
    % to disk (line 9), and mark them as clean (line 10).
    % If the flushing time does not exceed our time interval, the thread is put 
    % to sleep for the remaining time (lines 11-13). 
    % Then based on the state of the host at current simulated time, 
    % the algorithm continues or finishes the loop.
    Because periodical flushing is simulated as a background thread, it can happen concurrently
    with disk I/O initiated by the main thread. This parallelism is handled by the 
    storage model and reflected in simulated I/O time.

    \subsection{I/O Controller}
     
    \begin{algorithm}\caption{File chunk read simulation of IOController}
    \label{alg:read}
        \small
        \begin{algorithmic}[1]
            \Input
                \Desc{cs}{chunk size}
                \Desc{fn}{file name}
                \Desc{fs}{file size (assumed to fit in memory)}
                \Desc{mm}{MemoryManager object}
                \Desc{sm}{storage simulation model}
               \EndInput
               \State disk\_read = min(cs, fs - mm.cached(fn))
               \State cache\_read = cs - disk\_read
            \State mm.flush(cs + disk\_read - mm.free\_mem - mm.evictable, fn) 
            \State mm.evict(cs + disk\_read - mm.free\_mem, fn) 
            \If {disk\_read $>$ 0}  \Comment{Read uncached data}    
                \State sm.read(disk\_read)  
                \State mm.add\_to\_cache(disk\_read, fn)     
            \EndIf
            \If {cache\_read $>$ 0} \Comment{Read cached}
                \State mm.cache\_read(cache\_read)  
            \EndIf
        \end{algorithmic}
    \end{algorithm}
    As mentioned previously, our model reads and writes file chunks in a
    round-robin fashion. To read a file chunk, simulated applications send
    chunk read requests to the IOController which processes them using
    Algorithm~\ref{alg:read}. First, we calculate the amount of uncached
    data that needs to be read from disk, and the remaining amount is read
    from cache (line 7-8). The amount of memory required to read the chunk
    is calculated, corresponding to a copy of the chunk in anonymous memory
    and a copy of the chunk in cache \tristan{Create a variable
    "required\_mem" in the algorithm to clarify}.
    \tristan{add a call to mm in the algorithm to increment anonymous memory. Mention in the experiments that anonymous memory is flushed after each task.}
    If there is not enough available memory, the Memory Manager is called
    to flush dirty data (line 9). If necessary, flushing is complemented by
    eviction (line 10). When called with negative arguments, functions
    flush and evict simply return and do not do anything. Then, if there is
    uncached data, a disk read is simulated with disk model (line 12)
    \tristan{this is not consistent with the new Figure 1 where disk reads
    are done by the mm}, and this data of the file is added to cache (line
    13).
    Finally, if cached data needs to be read, the Memory Manager is called
    to simulate a cache read  
    and update the corresponding data blocks accordingly (line 16).

    \begin{algorithm}\caption{File chunk write simulation of IOController}
    \label{alg:write}
        \small
        \begin{algorithmic}[1]
            \Input
                \Desc{cs}{chunk size}
                \Desc{fn}{file name}
                \Desc{mm}{MemoryManager object}
                \Desc{sm}{storage simulation model}
               \EndInput
            \State remain\_dirty = dirty\_ratio * mm.avail\_mem - mm.dirty
            \If {remain\_dirty $>$ 0} \Comment{Write with memory bandwidth}
                \State mm.evict(min(cs, remain\_dirty) - mm.free\_mem)
                \State mem\_amt = min(cs, mm.free\_mem)
                \State mm.write(fn, mem\_amt) 
            \EndIf
            \If {mem\_amt $<$ cs}  \Comment{Write with disk bandwidth}
                \State mm.flush(cs - mem\_amt)  
                \State mm.evict(cs - mem\_amt  - mm.free\_mem) 
                \State mm.add\_to\_cache(fn, min(cs - mem\_amt, mm.free\_mem)) \tristan{why is data written to cache with "addtocache" here and with "write" at line 10?}
                \State sm.write(cs - mem\_amt) \tristan{According to Fig 1 the IOController is not supposed to interact directly with sm}
            \EndIf
            
        \end{algorithmic}
    \end{algorithm}
    Algorithm~\ref{alg:write} describes our simulation of chunk writes in 
    the IOController. 
    % Data is written to cache until the amount of dirty data in the cache 
    % exceeds the dirty ratio times the amount of available memory. Data is then written to disk.
    Our algorithm initially checks the  amount of data that 
    can be written as dirty data given the dirty ratio (line 6).
    If this amount is greater than 0, the Memory Manager is requested to evict 
    data from cache if necessary (line 8) \tristan{the difference between available and free memory should be explained somewhere}.
    After eviction, the amount of data that can be written to 
    page cache is calculated (line 9), and a cache write is simulated (line 10).
    To write the remaining data, we flush and 
    evict data as possible so that this data can be 
    written to cache (line 13-14). 
    The Memory Manager does not guarantee that the amount of flushed 
    and evicted data is enough to write the remaining data. 
    In the case it is less than the required amount, the remaining data 
    being written to cache can be flushed and evicted right away. 
    Thus, the amount of remaining data written and kept in cache 
    after the write is limited by the amount free memory, and this amount 
    is added to cache (line 15) \tristan{the previous two sentences are very unclear, I don't understand them so I can't reformulate.}. 
    Finally, the disk write is simulated with the remaining data amount (line 16), 
    and the chunk write algorithm finishes \tristan{see comment in algorithm}.
            
        \subsection{Implementation}

            To validate our simulation model, we created a simple prototype
            simulator independent of existing simulation frameworks and libraries. 
            This allowed us to evaluate the accuracy and correctness of our 
            model in a simple scenario before integrating it to the more complex 
            WRENCH framework. 
            In this prototype we used the following basic storage model for 
            both memory and disk: 
            \begin{align*}
                & t_{r} = D / b_r \\ 
                & t_{w} = D / b_w\
            \end{align*}        
            
            where:
            \begin{itemize}
                \item $t_{r}$ is the data read time
                \item $t_{w}$ is the data write time
                \item $D$ is the amount of data to read or write
                \item $b_r$ is the read bandwidth of the device
                \item $b_w$ is the write bandwidth of the device
            \end{itemize}

            Since bandwidth sharing was not simulated, this prototype does not support
            concurrency: it is limited to single-threaded applications running on systems
            with a single-core CPU. We used this prototype for a first validation of our simulation
            model against a real sequential application running on a real system.
            The source code this prototype is available at
            \url{https://github.com/big-data-lab-team/paper-io-simulation/tree/master/exp/pysim}. We used Python 3.7.

            We also implemented our model in WRENCH, separately from existing WRENCH components. 
            Page cache simulation is available through a user API and can be activated using a command-line argument.
            We used SimGrid 3.25 and WRENCH 1.6. SimGrid source code is available at \url{https://framagit.org/simgrid/simgrid},
            and WRENCH is at \url{https://github.com/wrench-project/wrench/tree/master/src/wrench/services/memory}.
            \tristan{which commit did you use?}

\tristan{anything else interesting to mention about the implementation?}

        \subsection{Experiments}

        Our experiments compare real executions with the original WRENCH,
        with our WRENCH-cache extension, and with our prototype Python simulator. We
        evaluate our page cache simulation in single-threaded and
        multi-threaded applications, accessing data on local and remote
        file systems.

        We simulate an application with three single-core, sequential tasks
        where each task reads the file produced by the previous task,
        increments every byte of this file to emulate real processing, and
        writes the resulting data to disk. We also use a real application
        \textcolor{red}{[Maybe briefly describe this application's purpose
        (e.g. a standard fMRI preprocessing piepeline)]}, to evaluate the
        applicability of our simulation model.

        The real application runs on a dedicated cluster hosted at
        Concordia University, with one login node, 9 compute nodes, and 4
        storage nodes connected with a network bandwidth of 25 Gbps. Each
        compute node has 2 $\times$ 16-core Intel(R) Xeon(R) Gold 6130 CPU
        @ 2.10GHz, 275~GB (256~GiB) \tristan{are you sure it's not 256GB?
        pls check.} of RAM, 6 $\times$ SSDs of 450~GB each with the XFS
        file system, 378~GB of tmpfs, and 126~GB of devtmpfs file system.
        Nodes run CentOS~8.1 with NFS version 4. We use the \texttt{atop}
        and \texttt{collectl} tools to monitor and collect memory status
        and disk throughput.

        To parametrize our simulators, we benchmarked the bandwidths of
        memory, local disk, remote disk (NFS), and network
        (Table~\ref{table:benchmark}). Since SimGrid only supports
        symmetrical bandwidths, we use the mean of the read and write
        bandwidth values in our experiments.
            \begin{table}[htbp]
            \centering
            \begin{tabularx}{\columnwidth}{|c
            |>{\centering\arraybackslash}X
            |>{\centering\arraybackslash}X
            |>{\centering\arraybackslash}X|}
            \hline
                Device  & Cluster (real) & Python simulator & WRENCH simulator\\
            \hline
                Memory read  & 6860    & 4812     & 4812\\
                Memory write & 2764    & 4812 & 4812\\
                Local disk read & 510 & 465 & 465\\
                Local disk write & 420 & 465     & 465\\
                Remote disk read & 515 & - & 445\\
                Remote disk write & 375 & - & 445\\
                Network bandwidth & 3000 & - & 3000\\
            \hline
            \end{tabularx}
            \caption{Bandwidth benchmarks (MBps) and simulator configurations.
            The bandwidths used in the simulations are the average of the read and write bandwidths
            measured in the cluster.
            Remote disk and network accesses are not simulated in the Python simulator. \tristan{table could be prettified}}
            \label{table:benchmark}
            \end{table}

            As our focus is on I/O rather than compute, we measured
            application task durations on a cluster node and used these
            durations in our simulation. For the Python simulator,
            execution times were directly used in the simulation. For
            WRENCH and WRENCH-cache, we determined the corresponding number
            of flops on a 1~Gflops CPU and used these values in the
            simulation (see Table~\ref{table:tasks} \tristan{add a table to describe execution times}). The final simulated
            platform and application used in our experiment are avaiable at
            \url{https://github.com/wrench-project/wrench/tree/ec6b43561b95977002258c0fe37a4ecad8f1d33f/examples/basic-examples/io-pagecache}.


            % Finally, because WRENCH simulates applications base on network-communication,
            % we use an infinite bandwidth to eliminate network latency for local I/Os.

            Our first experiment simply runs one instance of the
            application on a single cluster node, with different input file
            sizes (20~GB, 50~GB, 75~GB, 100~GB), and with all I/Os directed
            to the same local disk.

            Our second experiment runs concurrent instances of the
            application on a single node, all application instances
            operating on different files of size 3~GB stored in the same
            local disk, sharing the disk bandwidth. We vary the number of
            concurrent application instances from 1 to 32 since cluster
            nodes have 32 CPU cores.

            Our third experiment occurs in the same configuration as the
            previous one, albeit on a 400-GB NFS-mounted remote disk
            mounted from another compute node. \tristan{As is commonly
            configured in HPC environments to avoid data loss,?} NFS client
            and server read caches are enabled, but server writes are configured with
            write-through cache instead of write back \tristan{how about client write
            cache?}. Therefore, all the writes happen at disk bandwidth, but
            reads can benefit from cache hits. We simulate write-through by
            writing data directly to disk and adding them to the server
            cache \tristan{it it part of the IOController? it would deserve
            a few sentences in the methods.}.

            Finally, we simulate a real application from the neuroimaging
            domain. \textcolor{red}{[Description of the real pipeline with
            nighres (including the workflow engine if it's Dask)]}.
            We simulate this pipeline with simulators using original WRENCH
            and WRENCH-cache. Because our work focuses on I/O time, we
            assume that CPU time is correctly modeled and use the CPU time
            measured in the real pipelines to setup our simulated
            application.

            \tristan{mention that cache was flushed before each experiment}

    \section{Results}
    \label{results}

        \subsection{Single-threaded experiment}

        Figure~\ref{fig:single_error} shows the relative error of simulated
        read and write times throughout the three application tasks. The
        page cache simulation model drastically reduces simulation errors
        for all I/O operations except the first read that deals with
        uncached data. Errors are reduced from an average of \texttt{xxx\%}
        in WRENCH to \texttt{yyy \%} in the Python prototype and
        \texttt{zzz \%} in WRENCH-cache. The original WRENCH simulator significantly overestimates
        read and write times, due to its lack of page cache simulation
        model.

        % In this figure, \textit{file $<$i$>$} is the input of \textit{Read $<$i$>$} and
        % output of \textit{Write $<$i-1$>$}.
        With the input size of 20 GB, in both real execution and WRENCH-cache
        simulator, there is no difference in the page cache since all files in the
        pipeline can be entirely cached \tristan{what does this refer to? 
        For 20 GB there is still some error in WRENCH-cache, what do you mean ``there is no difference in the page cache''?}.

        With the input of 100 GB, the difference between real execution and WRENCH-cache
        simulator can be seen in the amount of \textit{file 3} cache after \textit{Write 2}.
        In real execution, after being written by \textit{Write 2},
        \textit{file 3} is entirely in cache.
        Thus, in \textit{Read 3}, \textit{file 3} is read with memory bandwidth.
        However, in the WRENCH-cache simulator, only a part of \textit{file 3} is
        cached after \textit{Write 2}, which means \textit{Write 2} reads a part of
        \textit{file 3} from disk.
        The reason for the difference is in our WRENCH-cache simulator,
        with 100 GB of input, while \textit{file 3} is being written,
        the page cache is saturated, eviction is then evoked to evict data
        from page cache and make space available for \textit{file 3}.
        However, because the evicted amount is less than required,
        some \textit{file 3} data is evicted after being written, make \textit{file 3}
        partially cached.
        As a result, in \textit{Read 3}, only a part of \textit{file 3} is read from cache
        with memory bandwidth, a part of it is read from disk with disk read bandwidth,
        which is many times slower, leading to longer read time.



            Memory profiling results of real pipeline execution and simulators 
            with 20GB and 100GB input files are illustrated in Fig.~\ref{fig:single_memprof}.
            Because the results with 20Gb, 50GB, 100GB shows a similar pattern,
            we only include the results with 20GB and 100GB in this paper.

            As shown in the figures, the makespans of the simulated pipeline are
            close to the makespan of the real execution with similar patterns of I/O time
            real execution and simulation.
            But more importantly, with all input sizes, similar trends can also be found
            in the lines describing memory status in simulators and the real execution.
            The similar evolutions of amount of cache used (purple lines) in reality
            and simulation show that the use of page cache is well reproduced by the simulators.
            The amount of dirty data (red lines) also follow similar trends,
            which means not only the amount dirty data, but also the flushing and
            periodical flushing mechanisms are accurately modeled in our simulators.
            This can be seen in in the increase of the amount of dirty data during the writes
            and the decrease during the reads and computations.
            According to the results achieved with all input sizes in the experiment,
            we can observe identical trends in reality and simulation with 20GB,
            50GB and 75GB input files. However, with 100GB input file,
            the third read is not correctly simulated, leading to longer simulated time
            than in real execution. This will be explained later in this section.



            \begin{figure*}
            \centering
            \begin{subfigure}{\linewidth}
                \centering
                   \includegraphics[width=\linewidth]{result/single/figures/single_errors.pdf}
            \end{subfigure}
            \caption{Simulation errors with different input file sizes}
            \label{fig:single_error}
            \begin{subfigure}{\linewidth}
                \centering
                   \includegraphics[width=\linewidth]{result/single/figures/single_memprof.pdf}
            \end{subfigure}
            \caption{Memory profiling results with different input file sizes.
            The bars in the figures present activities in tasks (read, compute and write),
            the lines describe memory status throughout run time of the pipeline.}
            \label{fig:single_memprof}
            \begin{subfigure}{\linewidth}
                \centering
                   \includegraphics[width=\linewidth]{result/single/figures/cached_files.pdf}
            \end{subfigure}
            \label{fig:single_cache}
               \caption{Amount of file data in cache after each I/O operation. 
               \textcolor{red}{Update real results of 20GB}}
            \end{figure*}

            In summary, in this single-threaded experiment, our model can accurately
            simulate I/O time as well as the internal memory behavior.
            This can help confirm that our model is on a right track and
            it can be further improved to achieve higher accuracy.

        \subsection{Multi-threaded experiment}

            \begin{figure*}
            \begin{subfigure}{\linewidth}
                \centering
                \includegraphics[width=\linewidth]{result/multi/figures/multi_local.pdf}
            \end{subfigure}
            \caption{I/O time of concurrent pipelines with local storage.}
            \label{fig:multi_local}
            \end{figure*}

            In this experiment, we run the real execution in 5 repetitions to
            capture the variability on the cluster.
            The results are analyzed in terms of average I/O time
            of concurrent pipelines shown in Fig.~\ref{fig:multi_local}.

            As is shown in the figure, when the number of pipelines increases,
            the average read time raises accordingly because the amount of data
            increases but memory and disk bandwidths are fixed and shared between pipelines.
            In comparison with original WRENCH, the results from WRENCH-cache are closer
            to reality.
            Especially, when it comes to the average write time,
            although the trend in reality is more complex, our WRENCH-cache
            simulator can still capture the pattern in real execution.
            In both, the average write time gradually increases before surging with the
            same slopes after the number of pipelines reaches 10.
            This can be explained that with less than 10 concurrent pipelines,
            the page cache is not saturated, so all files can be written entirely to
            cache with memory bandwidth in a short time.
            After the page cache is saturated with dirty data (at around
            10 concurrent pipelines in this experiment), this dirty data needs
            to be flushed in order to make space available for writing new data to cache.
            This dirty data is flushed to disk with disk bandwidth, which is much
            slower than memory bandwidth.
            The more pipelines we have, the more data needs to be written, the more data
            needs to be flushed, leading to a sharper increase in the average write time.

            To conclude, the results from WRENCH-cache simulator show that
            our model can also simulate I/O with page cache accurately in a
            multi-threaded application.

        \subsection{Remote storage}

            \begin{figure*}
            \begin{subfigure}{\linewidth}
                \centering
                \includegraphics[width=\linewidth]{result/multi/figures/multi_nfs.pdf}
            \end{subfigure}
            \caption{I/O time of concurrent pipelines with NFS}
            \label{fig:multi_nfs}
            \end{figure*}

            Similar to the previous experiment, the real execution with NFS is also
            repeated for 5 times and the results are described with the average I/O time
            shown in Fig.~\ref{fig:multi_nfs}.
            In general, the average I/O time reflect a very similar level of accuracy
            to the previous experiment results when the I/O time of pipelines
            is very close to the real execution.
            The simulated read time if a bit off when the number of pipelines
            surpasses 22, with 264 GB of data in total.
            This is due to the impact of simulated cache eviction when
            the page cache is saturated, which has been seen in
            Fig.~\ref{fig:single_error} with 100 GB of input.
            When it comes to write time, original WRENCH and WRENCH-cache
            results are identical when both are writing directly to disk.

        \subsection{Real application}

    \section{Discussion and Future Work}
    \label{discussion}
        In computing infrastructures, especially those use for data-intensive applications,
        page cache undoubtedly has certain impacts on performance.
        Conducting simulation experiments is an effective approach to predict,
        evaluate the performance of not only application, scheduler but also
        infrastructure settings.
        Unfortunately, page cache is missing in many simulation tools for HPC systems.
        In this study, we proposed a simulation model of I/O with page cache,
        implement it with SimGrid using WRENCH, and evaluate the results
        with different experiments.
        The results shows that our model can result in better simulation accuracy than
        the simulators in which page cache is not modeled.
        Furthermore, this level of accuracy can be achieved without a fine-grained
        level of details.
        This may create opportunities for the model to be applied in simulation of
        big data applications on HPC infrastructures, which leverage memory to
        employ page cache as a disk cache.
        Nevertheless, the model exposed some limitations that can be
        further improved.

        Our short term future work will be re-evaluate the model with asymmetrical
        disk bandwidths that will be released in the next SimGrid version.
        The next step is to study the detail in cache eviction mechanism to make
        the our simulation results closer to reality.
        Another point that needs to be further improved is the ability to configure
        different anonymous memory usage levels of applications since it affects
        the amount of cache used.
        We also plan to support random I/O and file readahead, as well as
        saving and restoring disk state when hosts are turned on/off.

\bibliographystyle{plain}
\bibliography{citation}

\end{document}
