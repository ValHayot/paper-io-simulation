\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}
\usepackage{hyperref}

\algblock{Input}{EndInput}
\algnotext{EndInput}
\algblock{Output}{EndOutput}
\algnotext{EndOutput}
\newcommand{\Desc}[2]{\State \makebox[2em][l]{#1}#2}

\newcommand{\tristan}[1]{\color{orange}\textbf{From Tristan:}#1\color{black}}
\newcommand{\valerie}[1]{\color{blue}\textbf{From Valerie:}#1\color{black}}


\begin{document}
\title{I/O simulation model with Linux page cache, integration and evaluation in SimGrid framework}

\author{\IEEEauthorblockN{Hoang-Dung Do, Val\'erie Hayot-Sasson, Tristan Glatard
  }\\
  \IEEEauthorblockA{
    Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada
  }
}

\maketitle

	\begin{abstract}
		\begin{itemize}
			\item The I/O bottleneck in HPC and the need of experiments.
			\item HPC experiment frameworks and advantages of SimGrid.
			\item The missing of the ability to simulate page cache, the goal of this paper.
			\item Principle of the simulator, experiment scenarios and comparisons.
			\item Brief discussion on results and future work.
		\end{itemize}
	\end{abstract}

	\section{Introduction}
		\begin{itemize}
			\item HPC, the bottleneck in I/O and the demand of HPC experiments. 
			\item Difficulties in conducting high performance computing experiments and the need of simulation frameworks.
			\item Existing experiment methods, simulators, simulation frameworks. The advantages of SimGrid compared to others \cite{simgrid2008, lebre2015}. The missing of the ability to simulate page cache in SimGrid \cite{lebre2015}.
			\item The objective of the paper: Add capability to simulate I/O with page cache in SimGrid.
			\item Clarify, distinguish between memory I/O (i/o to page cache) and disk I/O.
		\end{itemize}
	\section{Related Work}			
		
		\subsection{Page cache}
			\begin{itemize}
				\item What is page cache? How it works \cite{linuxdev3rd2010}. 
				Write-back vs writethrough and direct i/o. Effects and importance of page cache.
				\item Introduce some existing strategies with some highlighted pros and cons.
				\item Current implementation in Linux and some reasons why it is chosen to be implemented (implementation complexity, effectiveness, overhead, etc) \cite{linuxdev3rd2010}
				\item \tristan{LRU lists are mentioned in the next section, they should be described here.}
			\end{itemize}									

		\subsection{Simulators}
			\begin{itemize}
				\item Discuss some existing methods, simulation frameworks to conduct HPC experiments. Compare pros and cons (accuracy, simulation time, usability) of some simulators (SimGrid, GridSim).
				\item Related development: RAM energy consumption \cite{gill2019} \cite{ouarnoughi2017} 
				\item What is implemented in simulators that useful for our model:
				distinguished and shared bandwidths, concurrency, device latency. 
				\item Discuss the pros of SimGrid and the reasons why we chose it to extend. (Section 2.2.2 in \cite{simgrid2014})
				\item \tristan{Mention the status of storage simulation in simulators, so that we understand from Fig 1 that storage devices are available}
			\end{itemize}
			
	\section{Method}

		In this section, we present our approach to model file read and write with
		I/O to page cache, cache eviction and data flushing mechanisms implemented 
		in the Linux kernel. We also detail the design of our simulators and their
		implementation. Finally, we describe experimental scenarios to evaluate
		our simulation model on real applications using a standalone Python
		prototype, the current SimGrid simulator, and the SimGrid version that is 
		extended with our simulation model (SimGridExt).

		\subsection{Principle of the simulator}
	
            We separate our simulation model in two components, the IO
			Controller and the Memory Manager, which together simulate 
			file read/write mechanisms in Linux (Figure~\ref{fig:interaction}). 
			The Memory Manager simulates I/Os to cache, flushing, periodical flushing, 
			and cache eviction \tristan{how about periodical eviction?}, whereas the IO Controller simulates file reads and writes. 
			Simulated applications can simply interact with the IO Controller instead of 
			calling simulated storage directly as is the case with current simulators.

			\begin{figure}
   				\centering
   				\includegraphics[width=0.85\columnwidth]{figures/interaction.pdf}
   				\caption{Overview of the simulated write-back page cache}\label{fig:interaction}
			\end{figure}	

			\subsubsection{Memory simulation model}
			
			We leverage existing storage simulation models to simulate the memory,
			which is characterized by storage capacity, distinguished read/write 
			bandwidths and device latency. 
			All of these features are dealt within models in simulation 
			frameworks, which makes storage models applicable to memory \cite{lebre2015}. 
			In addition, leveraging existing disk models allows us 
			to take advantage of bandwidth sharing between concurrent disk I/O to 
			simulate parallel memory accesses.\tristan{we should refer to related work, 
            see comment there} \valerie{Agreed. It's unclear how existing storage models enable this at the moment}.

			Memory Manager simulates two parallel threads. The first thread is the 
			main thread where activities of applications are simulated, and responsible 
			for the simulation of data flushing, cache eviction and I/Os to cache. 
			The second thread is a simulated background thread, in which Memory Manager 
			periodically searches and flushes expired dirty data in page cache LRU lists 
			to disks. 

			In Linux kernel, the page cache LRU lists contain file pages. However, 
			due to the large number of file pages, managing lists of pages may 
			require significant computation overhead. 
			Thus, we introduce the concept of a data block as a unit to represent data 
			cached in memory. A data block is a subset of file pages stored in
            page cache that were accessed in the same read or write operation. 
			A data block has information about file name, block size, last access 
			time, and a dirty flag that represents whether the data is clean (0) 
			or dirty (1). 
			A given file can have multiple data blocks in page cache. In
			addition, a data block can be split into an arbitrary number of
			smaller blocks, and data blocks can be merged together.

			\begin{figure}
   				\centering
   				\includegraphics[width=\columnwidth]{figures/lru_lists.pdf}
   				\caption{Simulation of page cache LRU lists with data blocks}	\label{fig:lrulist}
			\end{figure}	
			
			Now, page cache LRU lists can be modeled with two lists of data blocks: 
			an active list and an inactive list, ordered by last access time 
			(earliest first, see Figure~\ref{fig:lrulist}).
			As in the kernel, our simulator keeps the size of the active list under
			twice the size of the inactive list by moving least recently 
            used data blocks from the active list to the inactive list.
			Modeling page cache LRU lists as lists of data blocks reduces the
			overhead of the simulator, as data blocks are obviously less
			numerous than file pages, while preserving the accuracy in I/O time
			simulation \tristan{how do you know that, can you provide an argument?}.
			
			Given an arbitrary file, at a specific time, its data can be partially 
			cached, completely cached, or uncached.
			Cached data can be composed of data blocks existing in either or both
            of the page cache LRU lists. Cached blocks reside in the inactive list
            unless they have been recently been reaccessed. Cached blocks 
            located on the inactive list that are subsequently accessed, 
            will be moved to the top of the active list. Cached blocks 
            recently written to cache are dirty until they are flushed to disk.

            The simulated LRU lists are used as follows. When a file is read, 
            as mentioned, its data can be partially cached, completely cached, 
            or uncached. 
            For cached data, there are existing data blocks representing
			this data in two page cache LRU lists. 
			As this cached data is accessed again, these blocks are moved to 
			the active list, dirty blocks are merged into a bigger dirty block, 
			while clean data blocks are merged into another clean block. 
			If data read is not in cache, a clean block representing uncached data 
			is created and put into the top of the inactive list. 
			
			In the case of writing data, we can assume that all written data 
			is uncached. After data is written, some can remain dirty, some is 
			clean after being flushed. 
			Thus, at most two data blocks can be added, one is clean and one 
			is dirty, to the inactive list.
			
			\textcolor{red}{[Is there any kind of accesses other than read and write? 
			Should we mention them?]}. 			
			
            As done in the kernel, we ensure the size of the active list does not 
            exceed twice the size of the inactive list 
			\cite{gorman2004understanding, linuxdev3rd2010}. 
			This is done by moving least recently used blocks in the active list 
			to the inactive list when the active list grows. If only a part of a data 
			block needs to be moved, the block is split.

			Next, we simulate data flushing, which runs on the main simulated 
			thread and writes back dirty data to disk. 
			The data flushing simulation function traverses the sorted inactive first, 
			and then the active list, looks for least recently used dirty blocks, 
			writes them to disk through the storage device model, and 
			sets dirty flags of these blocks to 0 until the total flushed amount 
			reaches the amount to be flushed or there is no dirty data left in cache. 
			If the last flushed block is not entirely flushed, it is split into 
			two blocks, one that is flushed and one that remains dirty.
			The flushing time is simulated with storage model.
				
			Similarly to data flushing, our cache eviction simulation runs on 
			the main thread, frees up the page cache by traversing and deleting 
			least recently used clean data blocks in the inactive list.
			The amount of evicted data is passed in and data blocks are deleted 
			from the inactive list until total evicted data reaches the amount 
			passed in, or there is no clean block left in the list.
			If the last evicted block is not entirely evicted, the block is split, 
			and only one block is deleted.
			Our cache eviction simulation does not add up to the simulated time 
			since cache eviction time is negligible in real systems.		
			
			\begin{algorithm}\caption{Periodical flushing simulation}\label{alg:pdflush}
				\small
				\begin{algorithmic}[1]
					\Input
        				\Desc{in}{page cache inactive list}
        				\Desc{ac}{page cache active list}
						\Desc{t}{predefined flushing time interval}
						\Desc{sm}{storage simulation model}
   					\EndInput
   					\While{has unfinished tasks or flushing}
						\State blocks = expired dirty blocks in active and inactive list
						\State flushing\_time = sm.write(blocks)
						\State Flag blocks as clean data
						\If{flushing\_time $<$ t}
							\State sleep(t - flushing\_time)
						\EndIf
					\EndWhile
				\end{algorithmic}
			\end{algorithm}				
			
			The periodical flushing simulation is run on a simulated background thread 
			to flush expired dirty data blocks from cache to disk.
			Similar to expired dirty pages in real Linux systems, a dirty block in our model 
			is considered expired if the time from its last access to current simulated time 
			is longer than the predefined expire time.
			The simulation algorithm of periodical flushing is an infinite loop in which 
			Memory Manager searches and flushes these expired dirty blocks to disk, 
			as described in Algorithm~\ref{alg:pdflush}. 
			The loop ends when the host does not has any remaining incomplete task 
			and all dirty data has been flushed (line 6). 
			In each repetition, Memory Manager collects expired dirty blocks in two 
			page cache LRU lists (line 7), simulates a write of this data to disk (line 8), 
			and mark these blocks as clean (line 9).
			If the time to flush data does not exceed our time interval, the thread is put 
			to sleep for the remaining time (lines 10-12). 
			Then simulation time is advanced, the algorithm finishes or continues the loop 
			based on the conditions at current simulated time.
			Because periodical flushing is simulated as a background thread, it can be 
			concurrent with disk read/write. This parallelism is handled by the 
			storage model and reflected in simulated I/O time. 	

			\subsubsection{File I/O simulation model}			
			
			Simulated applications can simply request for a file read 
			or a file write by calling the IO Controller instead of 
			the storage simulation models. 
			\tristan{this should move, not sure where for now: Because every computer has its own memory and system configurations, 
			one IO Controller object and one Memory Manager object are created 
			for each simulated machine.}
			
			To read or write a file, a simulated application sends a request to the 
			IO Controller instance on the node where the requested file is stored.
			Based on the memory status in Memory Manager, the IOController 
			orchestrates cache reads and writes, disk reads and writes, flushing, cache eviction 
			 and returns to the simulated application.
			
			\begin{algorithm}\caption{File read simulation}\label{alg:read}
				\small
				\begin{algorithmic}[1]
					\Input
        				\Desc{fn}{file name}
        				\Desc{fs}{file size (assumed to fit in memory)}
						\Desc{mm}{MemoryManager object}
						\Desc{sm}{storage simulation model}
   					\EndInput
					\State mm.flush(2*fs - mm.cached(fn) - mm.evictable\_mem) 
					\State mm.evict(2*fs - mm.cached(fn) - mm.free\_mem) 
					\If {mm.cached(fn) $>$ 0}  \Comment{Read file cached data}
    					\State cache\_read = mm.cache\_read(fn)  \Comment{Read async}
					\EndIf
					\If {mm.cached(fn) $<$ fs} \Comment{Read uncached data from disk}
						\State disk\_read = sm.read(fs - mm.cached(fn)) \Comment{Async disk read}
						\State mm.add\_to\_cache(fs - mm.cached(fn), fn)
					\EndIf					
					\State Wait for completion of cache\_read and disk\_read 
				\end{algorithmic}
			\end{algorithm}			
			
			The algorithm to simulate file reads is described in Algorithm~\ref{alg:read}.   
%			In Linux systems with page cache, data can be either read from cache 
%			or read from disk with disk bandwidth. 
%			Therefore read time can be considered as the sum of the time required to 
%			read from all devices\valerie{Can you clarify if disk+memory writes are not concurrent }, enabling us to generalize reading process as 
%			two separate phases, cache read and disk read. 
			In Linux systems with page cache, data can be either read from cache 
			or read from disk concurrently. As this concurrency is modeled in simulation 
			frameworks, we simulate a file read as follows.
			When a simulated application reads a file, it sends a file read request 
			to IO Controller. 
			First, if there is not enough memory available to store two
			copies of the file (one in cache, one in anonymous memory), the
			Memory Manager is called to flush dirty data to disk and simulate 
			flushing time with memory model (line 6).
			This flushing is complemented by eviction by the Memory Manager (line 7). 
			Here, we assume that user applications use the amount of memory 
			equal to the file size. 
			Next, if the file is cached, either partially or entirely, data is read from cache (line 8). 
			Memory Manager is called to simulate a asynchronous cache read, 
			update the corresponding data blocks in cache (line 9). 
			If file is not cached or partially cached, a disk read is required (line 11). 
			The storage model simulates a asynchronous disk read with the remaining 
            amount of data (line 12), and data read from disk is added to 
            cache (line 13). 
            At the end of the algorithm, we wait for the completion of two 
            asynchronous activities, the cache read in line 9 and the disk read in line 12, 
            to finish our file read and get the total simulated read time (line 15). 

			\begin{algorithm}\caption{File write simulation}\label{alg:write}
				\small
				\begin{algorithmic}[1]
					\Input
        				\Desc{fn}{file name}
        				\Desc{fs}{file size}
						\Desc{mm}{MemoryManager object}
						\Desc{sm}{storage simulation model}
   					\EndInput
					\State remain\_dirty = dirty\_ratio * mm.avail\_mem - mm.dirty
					\State mem\_amt = min(fs, remain\_dirty)
					\If {remain\_dirty $>$ 0} \Comment{Write with memory bandwidth}
    					\State mm.evict(mem\_amt - mm.free\_mem)
    					\State mm.write(fn, mem\_amt) 
    				\EndIf
					\If {mem\_amt $<$ fs}  \Comment{Write with disk bandwidth}
						\State mm.flush(fs - mem\_amt)  \Comment{Concurrent with disk write}
						\State mm.evict(fs - mem\_amt  - mm.free\_mem) 
						\State mm.add\_to\_cache(fn, min(fs - mem\_amt, mm.free\_mem))
						\State sm.write(fs - mem\_amt)
					\EndIf
					
				\end{algorithmic}
			\end{algorithm}

			Algorithm~\ref{alg:write} describes our simulation of file write. 
			During file write simulation, a file is cached in memory and also written 
			to disk. Data can be written to cache without waiting for flushing or 
			being throttled before the amount of dirty data reaches dirty\_ratio. 
			Thus, our algorithm initially checks the amount of data that can be written 
			with memory write bandwidth (line 6-7).
			If this amount is greater than 0, MemoryManager is asked to evict 
			data from cache to accommodate the written data if needed (line 10). 
			Then, cache write is simulated with memory model and data written is 
			added to page cache LRU lists (10).
			After dirty\_ratio is reached, dirty data has to be flushed before new data 
			is written to cache again. 
			We flush and evict data from cache as much as possible so that the 
			remaining data can be written to cache (line 13-14). 
			Here, MemoryManager does not guarantee that the amount of flushed 
			and evicted data is enough for the remaining data. In the case it is 
			less than the required amount and existing dirty data can not be flushed, 
			the remaining data being written to cache can be flushed and evicted right away. 
			Thus, the amount of remaining data written and kept in cache 
			after the write is limited by the amount free memory, and this amount 
			is added to cache (line 15). 
			Finally, the disk write is simulated with the remaining amount of 
			data (line 16), and the algorithm finishes with the total file write simulation time.
			
		\subsection{Implementation}

			To validate our simulation model, we create a simple prototype
			simulator independent of existing simulation frameworks and libraries. 
			This enables us to evaluate the accuracy and correctness of our 
			model in a simple scenario before integrating it in the more complex SimGrid environment. 
			In this prototype we use the following basic storage model, used for both memory and disk: 
			\begin{align*}
				& t_{r} = D / b_r \\ 
				& t_{w} = D / b_w\
			\end{align*}		
			
			where:
			\begin{itemize}
				\item $t_{r}$ is the data read time
				\item $t_{w}$ is the data write time
				\item $D$ is the amount of data to read/write
				\item $b_r$ is the read bandwidth of the device
				\item $b_w$ is the write bandwidth of the device
			\end{itemize}			

			Since bandwidth sharing is not simulated, this prototype doesn't support 
			concurrency: it is limited to single-threaded pipelines running on systems 
			with a single-core CPU. We use it in a first validation of our simulation 
			model, against a real sequential pipeline running on a real system.
			Source code of our Python prototype is available at 
            \url{https://github.com/big-data-lab-team/paper-io-simulation/tree/master/exp/pysim}
			
			Having our model validated, we create simulators for different use cases 
			using original SimGrid and our SimGridExt version. 
			To simulate our pipelines and system, we leverage 
			the Workflow Management System Simulation Workbench 
			(WRENCH~\cite{wrench2018}), a framework built over SimGrid.
		    We compare the results of the simulators with
			original SimGrid and extended SimGrid with the results of real
			pipelines on a real system. 
			
			\textcolor{red}{[Description, implementation of our SimGridExt]}. 
		
			We use Python 3.7 to implement the simple simulator, SimGrid 3.25
			and Wrench 1.6 for SimGrid simulators. SimGrid source code is
			available at \url{https://framagit.org/simgrid/simgrid}, and WRENCH
			is at \url{https://github.com/wrench-project/wrench}.
			
		\subsection{Experiments}
		
        	The goal of our experiments is to evaluate our write-back cache 
			simulation in single-threaded and multi-threaded applications
			accessing data on different types of file system. We use a pipeline
			of sequential tasks where each task reads an input file, increments
			every byte of the input to emulate real processing and avoid caching
			effects, and writes the result to disk. The output of the previous
			task is the input of the next task. We also use a real application 
            pipeline \textcolor{red}{[Maybe briefly describe this application's purpose (e.g. a standard fMRI preprocessing piepeline)]}, to evaluate the applicability of our simulation model. 
			
			We use our dedicated cluster hosted at Concordia University to conduct 
			the experiments. The cluster consists of one login node, 8 compute nodes 
			and 4 storage nodes connected with two network switches. The login node 
			has one Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz, 137~GB (128~GiB) of RAM, 
			1.8TB of storage of XFS file system, 13~GB of tmpfs file system and 385 TB of 
			Lustre shared file system. Each compute node has two 16 cores Intel(R) 
			Xeon(R) Gold 6130 CPU @ 2.10GHz, 275~GB (256~GiB) of RAM and 6 SSDs, 
			450~GB each with XFS file system, 378~GB of tmpfs and 126~GB of devtmpfs file system.
			Lustre file system is configured with one metadata target with 854~GB 
			of storage, 44 object storage targets with 8.7~TB of storage each. 
			The cluster is run on CentOS 8.1 with the Slurm Workload Manager installed. 
			We use \textbf{\textit{atop}} and \textbf{\textit{collectl}} as tools to 
			monitor and collect data of memory, page cache status and disk throughput 
			on the cluster. 
			\textcolor{red}{[Add network description here, include number of disks 
			on all nodes ,maybe add a diagram]}
			
			\subsubsection{Single-threaded evaluation}
			
			The simplest execution scenario is single-threaded, where memory 
			bandwidth and disk bandwidth are dedicated. Our goal is to 
			compare the detailed simulation results of each task in the pipeline 
			including read/write time, memory status and then validate the correctness 
			of the simulation model. 

			The pipeline used in this experiment consists of three tasks, 
			each task reads a file, increases every byte and writes output file 
			to local disk. 
			We use the input sizes of 20~GB, 50~GB, 75~GB and 100~GB, run 
			the pipeline on the cluster, measure the CPU time of the tasks  
			with each input and use it to simulate CPU time in the simulators .

			We simulate the pipeline with the Python prototype simulator, 
			a simulator implemented with original SimGrid and a simulator implemented 
			with SimGrid extended with our model.  
			
			\subsubsection{Multi-threaded evaluation}

			In the second experiment, we evaluate the simulator in a concurrent I/O, 
			bandwidth-sharing scenario with a multi-threaded application. 			
			We vary the number of concurrent pipelines running on a multi-core host.  
			
			All the task in the pipelines read input files from and write output files 
			to a local disk. 
			As a compute node of the cluster has 32 cores CPU and 256~GB of RAM,  
			we create 32 input files with the size of 3~GB each and vary the number of 
			concurrent pipelines from 1 to 32. 
			
			Because implementation of a multi-threaded simulator  in Python is 
			expensive, and we have our model integrated in SimGrid, 
			we only compare the results of simulators with original SimGrid, 
			SimGrid integrated with our model and and real pipelines in this 
			experiment. The results are compared in total makespan of the pipelines, 
			cumulative read time and cumulative write time.
			
			\subsubsection{Evaluation on storage types}
			
		    Since different storage types should also be taken into account, 
		    we conduct another experiment with the same scenario as 
		    in the multi-threaded experiment, except that files are stored on a 
		    shared file system instead of local disk. 
		    
		    \textcolor{red}{[Description of Lustre on the cluster and how it's 
		    simulated]}. 
		    
		    \subsubsection{Simulation of a real application}

		    Finally, we consider a real pipeline from the neuroimaging domain. 			
			\textcolor{red}{[Description of the real pipeline with nighres 
			(including the workflow engine if it's Dask)]}.  			
			
			We simulate this pipeline with a simulator with original SimGrid and 
			extended SimGrid. 
			The simulation results are then compared with the real results. 
			Because our work focuses on I/O time, we assume that CPU time is 
			correctly modeled and use the CPU time measured in the real pipelines 
			to setup our simulated workflow. The results are compared in terms of 
			read/write time and total makespan.

	\section{Results}
	
		\begin{itemize}

			\item Quantitative results: 
				\begin{itemize}
					\item Relative errors of simulation time and memory used compared to real results.
					\item Simulation time compared to baseline SimGrid.
				\end{itemize} 

			\item Ability of the model to generalize memory trends (dirty data, cache used) and disk throughput.

		\end{itemize}

	\section{Discussion and Future Work}
		\begin{itemize}
			\item Sensitivity of the simulator on the variation of memory and disk bandwidth. 
			\item Our model does not support random I/O and readahead.
			\item Disk state save/restore when hosts are turned on/off.
		\end{itemize}
	\section{CFI for cluster}
\bibliographystyle{plain}
\bibliography{citation}

\end{document}
